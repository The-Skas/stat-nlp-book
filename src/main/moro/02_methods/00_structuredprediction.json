{
  "name" : "Structured Prediction",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\nIn general there seems to be no emerging unified _theory of NLP_ to emerge, and most textbooks and courses explain NLP as \n\n> collection of problems, techniques, ideas, frameworks, etc. that really are not tied together in any reasonable way other than the fact that they have to do with NLP.\n>\n>  -- <cite>[Hal Daume](http://nlpers.blogspot.co.uk/2012/12/teaching-intro-grad-nlp.html)</cite>\n\nThat's not to say though that they aren't cross-cutting patterns, general best practices and recipes that reoccur frequently. One such reoccuring pattern I found in many NLP papers and systems is what I like to refer to as the _[structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) recipe_. <span class=\"summary\">But there are *recipes* that reoccur frequently, such as **Structured Prediction**</span>   \n\n<div class=\"newslide\"></div>\n\n<div class=\"book-start\"></div>\nThe general goal we address with this recipe is the following. We like to, given some input structure \\\\(\\x \\in \\Xs \\\\), predict a suitable output structure \\\\(\\y \\in \\Ys \\\\). In the context of NLP \\\\(\\Xs\\\\) may be the set of document, and \\\\(\\Ys\\\\) a set of document classes (e.g. sports and business). \\\\(\\Xs\\\\) may also be the set of French sentences, and \\\\(\\Ys\\\\) the set of English sentences. In this case each \\\\(\\y \\in \\Ys\\\\) is a _structured_ object (hence the use of bold face). This structured **output** aspect of the problem has profound consequences on the methods used to address it (as opposed to structure in the input, which one can deal with relatively straight-forwardly). Generally we are also given some _training set_ \\\\(\\train\\\\) which may contain input-output pairs \\\\((\\x,\\y)_i\\\\), but possibly also just input data (in unsupervised learning), annotated data but for a different task (multi-task, distant, weak supervision etc), or some mixture of it. \n\n<div class=\"slide-start\"></div>\n\n* Given given some input structure \\\\(\\x \\in \\Xs \\\\), such as a word, sentence, or document, \n* Predict an **output structure** \\\\(\\y \\in \\Ys \\\\), such as a class label, a sentence or syntactic tree.\n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\n\n<div class=\"book-start\"></div>\n\nWith the above ingredients the recipe goes as follows: \n\n 1. Define a parametrized _model_ \\\\(s_\\params(\\x,\\y)\\\\) that measures the _match_ of a given \\\\(\\x\\\\) and \\\\(\\y\\\\). This model builds in some of the background knowledge we have about the task. The model is also controlled by a set of real-valued parameters \\\\(\\params\\\\) that are usually too numerous to be hand-tuned.   \n 2. _Learn_ the parameters \\\\(\\params\\\\) from the training data \\\\(\\train\\\\), ideally such that performance on the task of choice is optimized. This learning step usually involves some _continuous optimization problem_ that serves as a surrogate for the task performance we like to maximize. \n 3. Given an input \\\\(\\x\\\\) find the highest-scoring (and hence best-matching) output structure $$ \\y^* = \\argmax_{\\y\\in\\Ys} s(\\x,\\y) $$ to serve as the prediction of the model. Given that most of the structures we care about in NLP are discrete, this task usually involves some _discrete optimization problem_ and is important not just at _test time_ when the model is applied, but often also during training (as, intuitively, we like to train the model such that it _predicts well_).   \n\n<div class=\"slide-start\"></div>\n\nthe recipe goes as follows: \n\n * Define a parametrized _model_ \\\\(s_\\params(\\x,\\y)\\\\) that measures the _match_ of a given \\\\(\\x\\\\) and \\\\(\\y\\\\). \n * _Learn_ the parameters \\\\(\\params\\\\) from the training data \\\\(\\train\\\\) (a _continuous optimization problem_).\n * Given an input \\\\(\\x\\\\) find the highest-scoring output structure $$ \\y^* = \\argmax_{\\y\\in\\Ys} s(\\x,\\y) $$ (a _discrete optimization problem_).  \n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\n<div class=\"book-start\"></div>\n\nYou will see examples of this recipe throughout the book, as well as frameworks and methods that make this recipe possible. It's worthwhile noting that good NLPers usually combine three skills in accordance with this recipe: 1. modelling, 2. continuous optimization and 3. discrete optimization. For the second and third some basic mathematical background is generally useful, for the first some understanding of the language phenomena you seek to model can be helpful. It's probably fair to say that modelling is the most important bit, and in practice this often shows through the fact that clever features (part of the model) beat clever optimization quite often. \n<div class=\"slide-start\"></div>\n\n<span class=\"summary\"> **Good NLPers** combine **three skills** in accordance with this recipe: \n\n* modelling,\n* continuous optimization and\n* discrete optimization.\n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\n### In this Book\nThe structured prediction recipe can be found in several places within this book:\n\n  * [Word-Based MT](/template/statnlpbook/01_tasks/02_wordmt)\n  * more to come.",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
