{
  "name" : "Maximum Likelihood Estimation",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The Maximum Likelihood Estimator (MLE) is one of the simplest ways, and often most intuitive way, to determine the parameters of a probabilistic models based on some training data. Under favourable conditions the MLE has several useful properties. On such property is consistency: if you sample enough data from a distribution with certain parameters, the MLE will recover these parameters with arbitrary precision. In our [structured prediction recipe](/template/statnlpbook/02_methods/00_structuredprediction) MLE can be seen as the most basic form of continuous optimization for parameter estimation. \n\nIn this section we will focus on MLE for _discrete distributions_ and _continuous parameters_. We will assume a distribution \\\\(\\prob_\\params(\\x)\\\\) with \\\\(\\x = (x_1,\\ldots, x_n)\\\\) that factorizes in the following way:\n\n\\begin{equation}\n  \\prob_\\params(\\x) = \\prod_i^n \\prob_\\params(x_i|\\phi_i(\\x)) \n                    = \\prod_i^n \\param_{x_i|\\phi_i(\\x)}\n\\end{equation}\n\nHere the functions \\\\(\\phi_i\\\\) provide a context to condition the probability of \\\\(x_i\\\\) with. For example, in a trigram language model this could be the bigram history for word \\\\(i\\\\), and hence \\\\(\\phi_i(\\x) = (x_{i-1},x_{i-2})\\\\). Notice that this function should not consider the variable \\\\(x_i\\\\) itself. \n\nThe Maximum Likelihood estimate \\\\(\\params^*\\\\) for this model, given some training data \\\\(\\train = (x_1,\\ldots, x_n)\\\\), is defined as the solution to the following optimization problem:\n\n\\begin{equation}\\label{eq:mle}\n  \\params^* = \\argmax_{\\params} \\prob_\\params(\\train) = \\argmax_{\\params} \\log \\prob_\\params(\\train) \n\\end{equation}\n\nHere the second equality stems from the monotonicity of the \\\\(\\log\\\\) function, and is useful because the \\\\(\\log\\\\) expression is easier to optimize. In words, the maximum likelihood estimate are the parameters that assign maximal probability to the training sample.  \n\nAs it turns out, the solution for \\\\(\\ref{eq:mle}\\\\) has a _closed form_: we can write the result as a direct function of \\\\(\\train\\\\) without the need of any iterative optimization algorithm. The result is simply:\n\n\\begin{equation}\\label{eq:counts}\n  \\param_{x|\\phi} = \\frac{\\counts{\\train}{x,\\phi}}{\\counts{\\train}{\\phi}}\n\\end{equation}\n\nwhere \\\\(\\counts{\\train}{x,\\phi}\\\\) is the number of times we have seen the value \\\\(x\\\\) paired with the context \\\\(\\phi\\\\) in the data \\\\(\\train\\\\), and \\\\(\\counts{\\train}{\\phi}\\\\) the number of times we have seen the context \\\\(\\phi\\\\).   \n\nNotice that in the same way we can represent the context of a variable using a function \\\\(\\phi\\\\), and hence map contexts to more coarse-grained equivalence classes, we can map the values \\\\(x_i\\\\) to a more coarse grained representation \\\\(\\gamma(x_i)\\\\). For example, in a language model we could decide to only care about the syntactic type (Verb, Noun, etc.) of a word and use \\\\(\\gamma(x) = \\mbox{syn-type}(x)\\\\). In this case the MLE only changes in the way we count: instead of counting the times we see \\\\(x\\\\) paired with the context \\\\(\\phi\\\\), we count how often we see \\\\(\\gamma\\\\) paired with the context \\\\(\\phi\\\\). \n",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\n#### Derivation\nIt is easy to derive the estimate for the discrete distributions described above. First let us reformulate the log-likelihood \\\\(L\\\\) in terms of dataset counts:\n\n\\begin{equation}\n  \\newcommand{\\duals}{\\boldsymbol{\\lambda}}\n  \\newcommand{\\lagrang}{\\mathcal{L}}\n  L(\\train,\\params) = \\log \\prob_\\params(\\train) \n            = \\sum_{x,\\phi} \\counts{\\train}{x,\\phi} \\log \\param_{x|\\phi}\n\\end{equation}\n\nNext, remember that we want, for a given \\\\(\\phi\\\\), the parameters \\\\(\\param_{\\cdot,\\phi}\\\\) to represent a conditional probability distribution \\\\(\\prob_\\params(\\cdot|\\phi)\\\\). This requires positivity (which fall out naturally later), and crucially: a normalization constraint. In particular, we need \\\\(\\sum_x \\param_{x,\\phi} = 1\\\\). \n\nWe hence have to solve a *constrained* optimization problem. A Standard technique to solve such problems relies on the notion of the *Lagrangian* \\\\(\\lagrang\\\\): a version of the objective in which constraints are added as soft constraints weighted by the *lagrange multipliers* \\\\(\\duals\\\\): \n\n\\begin{equation}\n  \\lagrang(\\params,\\duals) = L(\\train,\\params) + \\sum_\\phi \\lambda_\\phi (1 - \\sum_x \\param_{x|\\phi})\n\\end{equation}\n\nIf \\\\(\\params^*\\\\) is a solution to the original optimization problem then there exist a set of multipliers \\\\(\\duals^*\\\\) such that \\\\(\\params^*,\\duals^*\\\\) is a *stationary point* of \\\\(\\lagrang\\\\). By setting \\\\(\\nabla_\\params \\lagrang = 0\\\\) and \\\\(\\nabla_\\duals \\lagrang = 0\\\\) we can find such points. \n\nWe first set \\\\(\\nabla_\\params \\lagrang = 0\\\\):\n\n\\begin{equation}\n  \\frac{\\partial \\lagrang}{\\partial \\param_{x|\\phi}} = \\counts{\\train}{x,\\phi} \\frac{1}{\\param_{x|\\phi}} - \\lambda_\\phi = 0 \n\\end{equation}\n\nThis means that each parameter needs to be proportional to the count of its corresponding event:\n\n\\begin{equation}\n  \\param_{x|\\phi} = \\frac{\\counts{\\train}{x,\\phi}}{\\lambda_\\phi}\n\\end{equation}\n\nSetting set \\\\(\\nabla_\\duals \\lagrang = 0\\\\) will recover the original constraints: \\\\(\\sum_x \\param_{x|y} = 1\\\\). Plugging the above expression for \\\\( \\param_{x|\\phi}\\\\) into this constraint will give us \\\\(\\lambda_\\phi = \\sum_x \\counts{\\train}{x,\\phi} = \\counts{\\train}{\\phi}\\\\) and hence equation \\\\(\\ref{eq:counts}\\\\). Notice that there is only a single stationary point, and hence the parameters \\\\(\\params\\\\) at this point need to be the optimal ones.\n\n\n\n\n\n",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
