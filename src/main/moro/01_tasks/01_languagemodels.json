{
  "name" : "Language Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLanguage models (LMs) calculate the probability to see a given sequence of words, as defined through a [tokenization](/template/statnlpbook/01_tasks/00_tokenization) algorithm, in a given language or sub-language/domain/genre. For example, an English language model may assign a higher probability to seeing the sequence \"How are you?\" than to \"Wassup' dawg?\", and for a hip-hop language model this proportion may be reversed. <span class=\"summary\">Language models (LMs) calculate the probability to see a given sequence of words.</span>\n\n<div class=\"newslide\"></div>\nThere are several use cases for such models: \n\n* To filter out bad translations in machine translation.\n* To rank speech recognition output. \n* In concept-to-text generation.\n\n<div class=\"newslide\"></div>\nMore formally, a language model is a stochastic process that models the probability \\\\(\\prob(w_1,\\ldots,w_d)\\\\) of observing sequences of words \\\\(w_1,\\ldots,w_d\\\\). We can, without loss of generality, decompose the probability of such sequences into<span class=\"summary\">Without loss of generality</span>  \n\n$$\n\\prob(w_1,\\ldots,w_d) = \\prob(w_1) \\prod_{i = 2}^d \\prob(w_i|w_1,\\ldots,w_{i-1}).\n$$\n\nThis means that a language model can be defined by how it models the conditional probablity \\\\(\\prob(w_i|w_1,\\ldots,w_{i-1})\\\\) of seeing a word \\\\(w_i\\\\) after having seen the *history* of previous words \\\\(w_1,\\ldots,w_{i-1}\\\\). We also have to model the prior probability \\\\(\\prob(w_1)\\\\), but it is easy to reduce this prior to a conditional probability as well. <span class=\"summary\">We only need to model how words are generated based on a **history**.</span>\n\n<div class=\"newslide\"></div>\nIn practice it is common to define language models based on *equivalence classes* of histories instead of having different conditional distributions for each possible history. This overcomes sparsity and efficiency problems when working with full histories. <span class=\"summary\">In practice it is common to define language models based on **equivalence classes** of histories.</span>   \n\n\n<div class=\"newslide\"></div>\n### N-gram Language Models\nThe most common type of equivalence class relies on *truncating* histories \\\\(w_1,\\ldots,w_{i-1}\\\\) to length \\\\(n-1\\\\):\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i|w_{i-n},\\ldots,w_{i-1}).\n$$\nThat is, the probability of a word only depends on the last \\\\(n-1\\\\) previous words. We will refer to such model as a *n-gram language model*.\n\n<div class=\"newslide\"></div>\n###A Uniform Baseline LM\n\n*Unigram* models are the simplest 1-gram language models. That is, they model the conditional probability of word using the prior probability of seeing that word:\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\prob(w_i).\n$$\n\n<div class=\"newslide\"></div>\nTo setup datasets and as baseline for more complex language models, we first introduce the simplest instantituation of a unigram model: a *uniform* language model which assigns the same prior probability to each word. That is, given a *vocabulary* of words \\\\(\\vocab\\\\), the uniform LM is defined as:<span class=\"summary\">Given a *vocabulary* of words \\\\(\\vocab\\\\), the **uniform LM** is defined as:</span>\n\n$$\n\\prob(w_i|w_1,\\ldots,w_{i-1}) = \\frac{1}{|\\vocab|}.\n$$\n\n<div class=\"newslide\"></div>\nLet us \"train\" and test such a language model on the OHHLA corpus. First we need to load this corpus. Below we focus on a subset to make our code more responsive and to allow us to test models more quickly.",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import chapter.languagemodels.Util._\nimport corpora.OHHLA._\n\nval docs = JLive.allAlbums flatMap loadDir\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\nval train = words(trainDocs)\nval test = words(testDocs)\ntrain.take(35).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nWe can now create a uniform language model using a built-in constructor. Language models in this book implement the `LanguageModel` trait. ",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "trait LanguageModel {\n  def order:Int     \n  def vocab:Set[String]\n  def probability(word:String, history:String*):Double    \n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\nThe most important method we have to provide is `probability(word,history)` which returns the probability of a word given a history. Let us implement a uniform LM using this trait.<span class=\"summary\">Let us implement a uniform LM using this trait.</span>  ",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class UniformLM(vocab:Set[String]) extends LanguageModel {\n  def order = 1\n  def probability(word:String, history:String*) = \n    if (vocab(word)) 1.0 / vocab.size else 0.0\n}\nval vocab = train.toSet\nval baseline = UniformLM(vocab)\nbaseline.probability(\"call\")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\"]"
      }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Sampling\nIt is instructive and easy to sample language from a language model. In many, but not all, cases the more natural the generated language of an LM looks, the better this LM is. <span class=\"summary\">The quality of an LM can often be gauged by looking at its **samples**, but models with poorer samples can still be useful.</span>\n\n<div class=\"newslide\"></div>\nTo sample from an LM one simply needs to iteratively sample from the LM conditional probability over words, and add newly sampled words to the next history. The only challenge in implementing this is to sample from a categorical distribution over words. Here we provide this functionality via `sampleCategorical`. <span class=\"summary\">You can sample **word-by-word**, the current word based on previously sampled ones.</span>  \n",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.ArrayBuffer\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\n  val words = lm.vocab.toIndexedSeq\n  val result = new ArrayBuffer[String]\n  result ++= init\n  for (_ <- 0 until amount) {\n    val history = result.takeRight(lm.order - 1)  \n    val probs = words.map(lm.probability(_,history:_*))\n    result += words(sampleCategorical(probs))\n  }\n  result.toIndexedSeq\n} \nsample(baseline,Nil,10).mkString(\" \")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\"]"
      }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Evaluation\nHow do we determine the quality of an (n-gram) LM? One way is through *extrinsic* evaluation: assess how much the LM improves performance on *downstream tasks* such as machine translation or speech recognition. Arguably this is the most important measure of LM quality, but it can be costly as re-training such systems may take days, and when we seek to develop general-purpose LMs we may have to evaluate performance on several tasks. This is problematic when one wants to iteratively improve LMs and test new models and parameters. It is hence useful to find *intrinsic* means of evaluation that assess the stand-alone quality of LMs with minimal overhead. <span class=\"summary\">Most important for LM quality is their impact on **downstream tasks** in **extrinsic** evaluations. This can be expensive to evaluate, hence **intrinsic** evaluations can be useful.</span>   \n\n<div class=\"newslide\"></div>\nOne intrinsic way is to measure how well the LM plays the \"Shannon Game\": Predict what the next word in actual context should be, and win if your predictions match the words in an actual corpus. This can be formalized  using the notion of *perplexity* of the LM on a given dataset. Given a test sequence \\\\(w_1,\\ldots,w_T\\\\) of \\\\(T\\\\) words, we calculate the perplexity \\\\(\\perplexity\\\\) as follows: <span class=\"summary\">The **perplexity** of an LM on a sample \\\\(w_1,\\ldots,w_T\\\\) is a measure of intrinsic quality:</span>  \n\n$$\n\\perplexity(w_1,\\ldots,w_T) = \\prob(w_1,\\ldots,w_T)^{-\\frac{1}{T}} = \\sqrt[T]{\\prod_i^T \\frac{1}{\\prob(w_i|w_{i-n},\\ldots,w_{i-1})}}\n$$\n\n<div class=\"newslide\"></div>\nWe can implement a perplexity function based on the `LanguageModel` interface. ",
      "extraFields" : { }
    }
  }, {
    "id" : 9,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def perplexity(lm:LanguageModel, data:Seq[String]) = {\n  var logProb = 0.0\n  val historyOrder = lm.order - 1\n  for (i <- historyOrder until data.length) {\n    val history = data.slice(i - historyOrder, i)\n    val word = data(i)\n    val p = lm.probability(word,history:_*)\n    logProb += math.log(p)\n  }\n  math.exp(-logProb / (data.length - historyOrder))\n} ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet's see how the uniform model does on our test set. ",
      "extraFields" : { }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(baseline, test)   ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Out-of-Vocabularly Words\nThe problem in the above example is that the baseline model assigns zero probability to words that are not in the vocabulary. Test sets will usually contain such words, and this leads to the above result of infinite perplexity.<span class=\"summary\">The model assigns 0 probability to words not in the training vocabulary.</span> \n",
      "extraFields" : { }
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \"]"
      }
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### The Long Tail\nThe fact that we regularly encounter new words in our corpus is a common phenomenon not specific to our corpus. Generally we will see a few words that appear repeatedly, and a long tail of words that appear a few times. While each individual long-tail word is rare, the probability of seeing any long-tail word is quite high (the long tail covers a lot of the frequency mass).<span class=\"summary\"> A few words appear repeatedly, and a **long tail** of words appear a few times but often in aggregate.</span> \n\n<div class=\"newslide\"></div>\nLet us observe this phenomenon for our data: we will rank the words according to their frequency, and plot this frequency against the rank.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 15,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \nlineplot(ranks, logX = false, logY = false) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \"]"
      }
    }
  }, {
    "id" : 16,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nIn log-space such rank vs frequency graphs resemble linear functions. This observation is known as *Zipf's Law*, and can be formalized as follows. Let \\\\(r\\_w\\\\) be the rank of a word \\\\(w\\\\), and \\\\(f\\_w\\\\) its frequency, then we have:<span class=\"summary\"> **Zipf's Law** states that word frequency \\\\(f\\_w\\\\) is inversely proportional to word rank \\\\(r\\_w\\\\):</span>  \n\n$$\n  f_w \\propto \\frac{1}{r_w}.\n$$\n\n<div class=\"newslide\"></div>\n#### Inserting Out-of-Vocabularly Tokens\nThe long tail of infrequent words is a problem for LMs because it means there will always be words with zero counts in your training set. There are various solutions to this problem. For example, when it comes to calculating the LM perplexity we could remove words that do not appear in the training set. This overcomes the problem of infinite perplexity but doesn't solve the actual issue: the LM assigns too low probability to unseen words. Moreover, the problem only gets worse when one considers n-gram models with larger \\\\(n\\\\), because these will encounter many unseen n-grams, which, when removed, will only leave small fractions of the original sentences. <span class=\"summary\">The **long tail** of infrequent words is a problem for LMs and NLP in general. It gets even worse for n-grams.</span> \n\n<div class=\"newslide\"></div>\nThe principled solution to this problem is smoothing, and we will discuss it in more detail later. Before we get there we present a simple preprocessing step that generally simplifies the handling of unseen words, and gives rise to a simple smoothing heuristic. Namely, we replace unseen words in the test corpus with an out-of-vocabularly token, say `OOV`. This means that LMs can still work with a fixed vocabularly that consists of all training words, and the `OOV` token. Now we just need a way to estimate the probability of the `OOV` token to avoid the infinite perplexity problem. <span class=\"summary\">One solution is to replace unseen words with an `OOV` token in the test set.</span> \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 17,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "replaceOOVs(OOV, baseline.vocab, test.take(10))",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \"]"
      }
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\n<div class=\"newslide\"></div>\nA simple way to (heuristically) estimate the `OOV` probability is to replace the first encounter of each word in the training set with the `OOV` token. Now we can estimate LMs as before, and will automatically get some estimate of the `OOV` probability. The underlying assumption of this heuristic is that the probability of unseen words is identical to the probability of encountering a new word. We illustrate the two operations of this method in the code below. <span class=\"summary\">To get probability estimates for these tokens we can replace the first encounter of each word in the training set with `OOV`.</span>  ",
      "extraFields" : { }
    }
  }, {
    "id" : 19,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "injectOOVs(OOV, Seq(\"A\",\"A\",\"B\",\"B\",\"A\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\"]"
      }
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<!--\n<div class=\"newslide\"></div>\n<div class=\"exercise\">\n  <div class=\"exname\">Exercise</div>\n  <div class=\"extext\">For which words do the underlying assumptions of the \"replace the first encounter\" heuristic hold?</div>\n</div>\n-->\n\n<div class=\"newslide\"></div>\nNow we can apply this to our training and test set, and create a new uniform model.",
      "extraFields" : { }
    }
  }, {
    "id" : 21,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = injectOOVs(OOV, words(trainDocs))\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\nval vocab = train.toSet \nval baseline = UniformLM(vocab)\nperplexity(baseline,test) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \"]"
      }
    }
  }, {
    "id" : 22,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Training Language Models\nThe uniform LM is obviously not good at modelling actual language. To improve upon this baseline, we can estimate the conditional n-gram distributions from the training data. To this end let us first introduce one parameter \\\\(\\param_{w,h}\\\\) for each word \\\\(w\\\\) and history \\\\(h\\\\) of length \\\\(n - 1\\\\), and define a parametrized language model \\\\(p_\\params\\\\): <span class=\"summary\">Let us define a parametrized language model \\\\(p_\\params\\\\):</span>\n\n$$\n\\prob_\\params(w|h) = \\param_{w,h}\n$$\n\n<div class=\"newslide\"></div>\nTraining an n-gram LM amounts to estimating \\\\(\\params\\\\) from some training set \\\\(\\train=(w_1,\\ldots,w_n)\\\\).\nOne way to do this is to choose the \\\\(\\params\\\\) that maximizes the log-likelihood of \\\\(\\train\\\\):\n$$\n\\params^* = \\argmax_\\params \\log p_\\params(\\train)\n$$\n\n<div class=\"newslide\"></div>\nAs it turns out, this maximum-log-likelihood estimate (MLE) can calculated in closed form, simply by counting:\n$$\n\\param^*_{w,h} = \\frac{\\counts{\\train}{h,w}}{\\counts{\\train}{h}} \n$$\n\nwhere \n\n$$\n\\counts{D}{e} = \\text{Count of } e \\text{ in }  D \n$$\n\n<div class=\"newslide\"></div>\nMany LM variants can be implemented simply by estimating the counts in the nominator and denominator differently. We therefore introduce a trait for such count-based LMs. This will help us later to implement LM variants by modifying the counts of a base-LM. <span class=\"summary\">Many LMs can be implemented by estimating the counts in the nominator and denominator differently, so let's define a corresponding `trait`</span>\n",
      "extraFields" : { }
    }
  }, {
    "id" : 23,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "trait CountLM extends LanguageModel {\n  def counts:List[String] => Double\n  def norm:List[String] => Double\n  def probability(word:String, history:String*) = {\n    val subHistory = history.takeRight(order - 1).toList\n    counts(word :: subHistory) / norm(subHistory)\n  }\n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 24,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us use this to code up a generic NGram model",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.HashMap\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\n  val vocab = train.toSet\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\n  for (i <- order until train.length) {\n    val history = train.slice(i - order + 1, i).toList\n    val word = train(i)\n    counts(word :: history) += 1.0\n    norm(history) += 1.0\n  }\n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us train a unigram model.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val unigram = NGramLM(train,1) \nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\"]"
      }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThe unigram LM has substantially reduced (and hence better) perplexity:",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(unigram,test) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \"]"
      }
    }
  }, {
    "id" : 30,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us also look at the language the unigram LM generates.",
      "extraFields" : { }
    }
  }, {
    "id" : 31,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "sample(unigram, Nil, 10).mkString(\" \") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \"]"
      }
    }
  }, {
    "id" : 32,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Bigram LM\n\nThe unigram model ignores any correlation between consecutive words in a sentence. The next best model to overcome this shortcoming is a bigram model. This model conditions the probability of the current word on the previous word. Let us construct such model from the training data. <span class=\"summary\">The **Bigram** model conditions the probability of the current word on the previous word.</span>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 33,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val bigram = NGramLM(train, 2)\nbarChart(vocab.map(w => w -> bigram.probability(w,\"I\")).toSeq.sortBy(-_._2).take(10)) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \"]"
      }
    }
  }, {
    "id" : 34,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nYou can see a more peaked distribution conditioned on \"I\" than in the case of the unigram model. Let us see how the bigram LM generates language. <span class=\"summary\">Let us see how the bigram LM generates language.</span>",
      "extraFields" : { }
    }
  }, {
    "id" : 35,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "sample(bigram, List(\"[BAR]\"), 10).mkString(\" \") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \"]"
      }
    }
  }, {
    "id" : 36,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nDoes the bigram model improve perplexity?",
      "extraFields" : { }
    }
  }, {
    "id" : 37,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(bigram,test) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \"]"
      }
    }
  }, {
    "id" : 38,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nUnfortunately the bigram model has the problem we tried to avoid using the OOV preprocessing method above. The problem is that there are contexts in which the OOV word (and other words) hasn't been seen, and hence it receives 0 probability. <span class=\"summary\">While every word has been seen, it may not have been seen in every **context**.</span>\n",
      "extraFields" : { }
    }
  }, {
    "id" : 39,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "bigram.probability(\"[OOV]\",\"money\") ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \"]"
      }
    }
  }, {
    "id" : 40,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n### Smoothing\n\nThe general problem is that maximum likelhood estimates will always underestimate the true probability of some words, and in turn overestimate the (context-dependent) probabilities of other words. To overcome this issue we aim to _smooth_ the probabilities and move mass from seen events to unseen events.\n\n<div class=\"newslide\"></div>\n#### Laplace Smoothing\n\nThe easiest way to overcome the problem of zero probabilities is to simply add pseudo counts to each event in the dataset (in a Bayesian setting this amounts to a maximum posteriori estimate under a dirichlet prior on parameters). <span class=\"summary\">Add **pseudo counts** to each event in the dataset.</span> \n\n$$\n\\param^{\\alpha}_{w,h} = \\frac{\\counts{\\train}{h,w} + \\alpha}{\\counts{\\train}{h} + \\alpha \\lvert V \\rvert } \n$$\n\n<div class=\"newslide\"></div>\nLet us implement this in Scala.",
      "extraFields" : { }
    }
  }, {
    "id" : 41,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class LaplaceLM(base:CountLM, alpha:Double) extends CountLM {\n  def vocab = base.vocab\n  def order = base.order\n  def counts = h => base.counts(h) + alpha\n  def norm = h => base.norm(h) + alpha * base.vocab.size\n}\nval laplaceBigram = LaplaceLM(bigram, 0.1) \nlaplaceBigram.probability(\"[OOV]\",\"money\")",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \"]"
      }
    }
  }, {
    "id" : 42,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThis should give a better perplexity value:",
      "extraFields" : { }
    }
  }, {
    "id" : 43,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "perplexity(laplaceBigram, test)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM, alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram, 0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\"]"
      }
    }
  }, {
    "id" : 44,
    "compiler" : "html",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"exercise\">\n  <div class=\"exname\">Exercise 2</div>\n  <div class=\"extext\">Can you find a better pseudo-count number?</div>\n</div>\n",
      "extraFields" : { }
    }
  }, {
    "id" : 45,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Adjusted counts\nIt is often useful to think of smoothing algorithms as un-smoothed Maximum-Likelhood estimators that work with *adjusted* n-gram counts in the numerator, and fixed history counts in the denominator. This allows us to see how counts from high-frequency words are reduced, and counts of unseen words increased. If these changes are too big, the smoothing method is likely not very effective. <span class=\"summary\">Good to think of smoothing as moving mass and **adjusting** the counts in the **numerator**.</span>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 46,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n\nLet us reformulate the laplace LM using adjusted counts. Note that we since we have histories with count 0, we do need to increase the original denominator by a small \\\\(\\epsilon\\\\) to avoid division by zero. \n$$\n\\begin{split}\n\\counts{\\train,\\alpha}{h,w} &= \\param^{\\alpha}_{w,h} \\cdot (\\counts{\\train}{h} +  \\epsilon)\\\\\\\\\n\\counts{\\train,\\alpha}{h} &= \\counts{\\train}{h} + \\epsilon\n\\end{split}\n$$\n<div class=\"newslide\"></div>",
      "extraFields" : { }
    }
  }, {
    "id" : 47,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class LaplaceLM(base:CountLM, alpha:Double) extends CountLM {\n  val eps = 0.001\n  def vocab = base.vocab\n  def order = base.order\n  def counts = h => \n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\n  def norm = h => base.norm(h) + eps\n}\nval laplaceBigram = LaplaceLM(bigram, 0.1)\nbigram.counts(\"[OOV]\"::\"[OOV]\":: Nil) -> laplaceBigram.counts(\"[OOV]\"::\"[OOV]\":: Nil)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel, init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    val subHistory = history.takeRight(order - 1).toList\\n    counts(word :: subHistory) / norm(subHistory)\\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String], order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String], Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String], Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train, 2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM, alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram, 0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram, test)\"]"
      }
    }
  }, {
    "id" : 48,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nWe see above that for high frequency words the absolute counts are altered quite substantially. This is unfortunate because for high frequency words we would expect the counts to be relatively accurate. Can we test more generally wether our adjusted counts are sensible? <span class=\"summary\">Can we test more generally wether our adjusted counts are sensible?</span>\n\n<div class=\"newslide\"></div>\nOne option is to compare the adjusted counts to average counts in a held-out set. For example, for words of count 0 in the training set, how does their average count in the held-out set compare to their adjusted count in the smoothed model? <span class=\"summary\">Compare adjusted counts to counts in a held out set:</span>   \n",
      "extraFields" : { }
    }
  }, {
    "id" : 49,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\n  val avgTestCounts = new HashMap[Double, Double] withDefaultValue 0.0\n  val norm = new HashMap[Double, Double] withDefaultValue 0.0\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\n    val trainCount = trainLM.counts(ngram)\n    val testCount = testLM.counts(ngram)\n    avgTestCounts(trainCount) += testCount\n    norm(trainCount) += 1.0\n  }\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\n  avgTestCounts\n}",
      "extraFields" : {
        "hide" : "true",
        "hide_output" : "true",
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\"]"
      }
    }
  }, {
    "id" : 50,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val testBigram = NGramLM(test,2)\nval jointVocab = (train ++ test).toSet\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\nval avgLaplaceCounts = avgCounts(bigram, LaplaceLM(bigram,0.1),jointVocab)\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i), avgLaplaceCounts(i)) \ntable(Seq(\"train-count\",\"test-count\",\"smoothed-count\") +:compared)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\"]"
      }
    }
  }, {
    "id" : 51,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Interpolation\nFor a given context the smoothing methods discussed above shift mass uniformly across the words that haven't been seen in this context. This makes sense when the words are not in the vocabularly. However, when words are in the vocabularly but just have not been seen in the given context, we can do better because we can leverage statistics about the word from other contexts. In particular, we can *back-off* to the statistics of \\\\(n-1\\\\) grams. <span class=\"summary\">When we don't have reliable \\\\(n\\\\)-gram counts we should use \\\\(n-1\\\\)-gram counts.</span> ",
      "extraFields" : { }
    }
  }, {
    "id" : 52,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//find bigrams for which the difference between the unigram probability is maximal\nval differences = for (w1 <- vocab.view if w1 != OOV; w2 <- vocab.view) \n  yield (w1,w2) -> (unigram.probability(w1) - bigram.probability(w1,w2))\ndifferences.maxBy(_._2)",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"train-count\\\",\\\"test-count\\\",\\\"smoothed-count\\\") +:compared)\"]"
      }
    }
  }, {
    "id" : 53,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nA simple technique to use the \\\\(n-1\\\\) gram statistics is interpolation. Here we  compose the probability of a word as the weighted sum of the probability of an \\\\(n\\\\)-gram model \\\\(p'\\\\) and a back-off \\\\(n-1\\\\) model \\\\(p''\\\\): \n\n$$\n\\prob_{\\alpha}(w_i|w_{i-n},\\ldots,w_{i-1}) = \\alpha \\cdot \\prob'(w_i|w_{i-n},\\ldots,w_{i-1}) + (1 - \\alpha) \\cdot \\prob''(w_i|w_{i-n+1},\\ldots,w_{i-1})\n$$\n\n<div class=\"newslide\"></div>",
      "extraFields" : { }
    }
  }, {
    "id" : 54,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class InterpolatedLM(main:LanguageModel, backoff:LanguageModel, alpha:Double) extends LanguageModel {\n  def order = main.order\n  def vocab = main.vocab\n  def probability(word:String, history:String*) = \n    alpha * main.probability(word,history:_*) +\n    (1 - alpha) * backoff.probability(word, history.drop(1):_*)\n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"train-count\\\",\\\"test-count\\\",\\\"smoothed-count\\\") +:compared)\",\"//find bigrams for which the difference between the unigram probability is maximal\\nval differences = for (w1 <- vocab.view if w1 != OOV; w2 <- vocab.view) \\n  yield (w1,w2) -> (unigram.probability(w1) - bigram.probability(w1,w2))\\ndifferences.maxBy(_._2)\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 55,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us interpolate between a bigram and uniform language model, varying the interpolation parameter \\\\(\\alpha\\\\).\n",
      "extraFields" : { }
    }
  }, {
    "id" : 56,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val results = for (alpha <- (0 until 20).map(_ / 20.0)) yield \n  alpha -> perplexity(InterpolatedLM(bigram,unigram,alpha), test)\nlineplot(results) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"train-count\\\",\\\"test-count\\\",\\\"smoothed-count\\\") +:compared)\",\"//find bigrams for which the difference between the unigram probability is maximal\\nval differences = for (w1 <- vocab.view if w1 != OOV; w2 <- vocab.view) \\n  yield (w1,w2) -> (unigram.probability(w1) - bigram.probability(w1,w2))\\ndifferences.maxBy(_._2)\",\"case class InterpolatedLM(main:LanguageModel, backoff:LanguageModel, alpha:Double) extends LanguageModel {\\n  def order = main.order\\n  def vocab = main.vocab\\n  def probability(word:String, history:String*) = \\n    alpha * main.probability(word,history:_*) +\\n    (1 - alpha) * backoff.probability(word, history.drop(1):_*)\\n}\"]",
        "hide_output" : "false"
      }
    }
  }, {
    "id" : 57,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Backoff \nInstead of combining probabilities for all words given a context, it makes sense to back-off only when no counts for a given event are available and rely on available counts where possible. \n\n<div class=\"newslide\"></div>\nA particularly simple, if not to say stupid, backoff method is [Stupid Backoff](http://www.aclweb.org/anthology/D07-1090.pdf). Let \\\\(w\\\\) be a word and \\\\(h_{n}\\\\) be an n-gram of length \\\\(n\\\\):  \n\n$$\n\\prob_{\\mbox{Stupid}}(w|h_n) = \n\\begin{cases}\n\\frac{\\counts{\\train}{h_n,w}}{\\counts{\\train}{h_n}}  &= \\mbox{if }\\counts{\\train}{h_n,w} > 0 \\\\\\\\\n\\prob_{\\mbox{Stupid}}(w|h_{n-1}) & \\mbox{otherwise}\n\\end{cases}\n$$\n\n<div class=\"newslide\"></div>",
      "extraFields" : { }
    }
  }, {
    "id" : 58,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class StupidBackoff(main:CountLM, backoff:LanguageModel, alpha:Double) \n  extends LanguageModel {\n    def order = main.order\n    def vocab = main.vocab\n    def probability(word:String, history:String*) = \n      if (main.counts(word :: history.toList) > 0) main.probability(word, history:_*)\n      else alpha * backoff.probability(word, history.drop(1):_*)\n      \n}",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"train-count\\\",\\\"test-count\\\",\\\"smoothed-count\\\") +:compared)\",\"//find bigrams for which the difference between the unigram probability is maximal\\nval differences = for (w1 <- vocab.view if w1 != OOV; w2 <- vocab.view) \\n  yield (w1,w2) -> (unigram.probability(w1) - bigram.probability(w1,w2))\\ndifferences.maxBy(_._2)\",\"case class InterpolatedLM(main:LanguageModel, backoff:LanguageModel, alpha:Double) extends LanguageModel {\\n  def order = main.order\\n  def vocab = main.vocab\\n  def probability(word:String, history:String*) = \\n    alpha * main.probability(word,history:_*) +\\n    (1 - alpha) * backoff.probability(word, history.drop(1):_*)\\n}\",\"val results = for (alpha <- (0 until 20).map(_ / 20.0)) yield \\n  alpha -> perplexity(InterpolatedLM(bigram,unigram,alpha),test)\\nlineplot(results) \"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 59,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nIt turns out that the Stupid LM is very effective when it comes to *extrinsic* evaluations, but it doesn't represent a valid probability distribution: when you sum over the probabilities of all words given a history, the result may be larger than 1. This is the case because the main n-gram model probabilities for all non-zero count words already sum to 1. The fact that the probabilities sum to more than 1 makes perplexity values meaningless. The code below illustrates the problem.",
      "extraFields" : { }
    }
  }, {
    "id" : 60,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val stupid = StupidBackoff(bigram,unigram, 0.1)\nstupid.vocab.toSeq.map(w => stupid.probability(w, \"the\")).sum ",
      "extraFields" : {
        "aggregatedCells" : "[\"import chapter.languagemodels.Util._\\nimport corpora.OHHLA._\\n\\nval docs = JLive.allAlbums flatMap loadDir\\nval (trainDocs, testDocs) = docs.splitAt(docs.length / 2)\\nval train = words(trainDocs)\\nval test = words(testDocs)\\ntrain.take(35).mkString(\\\" \\\")\",\"trait LanguageModel {\\n  def order:Int     \\n  def vocab:Set[String]\\n  def probability(word:String, history:String*):Double    \\n}\",\"case class UniformLM(vocab:Set[String]) extends LanguageModel {\\n  def order = 1\\n  def probability(word:String, history:String*) = \\n    if (vocab(word)) 1.0 / vocab.size else 0.0\\n}\\nval vocab = train.toSet\\nval baseline = UniformLM(vocab)\\nbaseline.probability(\\\"call\\\")\",\"import scala.collection.mutable.ArrayBuffer\\ndef sample(lm:LanguageModel,init:Seq[String], amount:Int) = {\\n  val words = lm.vocab.toIndexedSeq\\n  val result = new ArrayBuffer[String]\\n  result ++= init\\n  for (_ <- 0 until amount) {\\n    val history = result.takeRight(lm.order - 1)  \\n    val probs = words.map(lm.probability(_,history:_*))\\n    result += words(sampleCategorical(probs))\\n  }\\n  result.toIndexedSeq\\n} \\nsample(baseline,Nil,10).mkString(\\\" \\\")\",\"def perplexity(lm:LanguageModel, data:Seq[String]) = {\\n  var logProb = 0.0\\n  val historyOrder = lm.order - 1\\n  for (i <- historyOrder until data.length) {\\n    val history = data.slice(i - historyOrder, i)\\n    val word = data(i)\\n    val p = lm.probability(word,history:_*)\\n    logProb += math.log(p)\\n  }\\n  math.exp(-logProb / (data.length - historyOrder))\\n} \",\"perplexity(baseline, test)   \",\"test.filterNot(train.toSet).take(3) map (w => w -> baseline.probability(w)) \",\"val counts = train.groupBy(identity).mapValues(_.length).toSeq.sortBy(-_._2)\\nval ranks = counts.map(_._2).zipWithIndex.map(p => (p._2.toDouble + 1) -> p._1.toDouble) \\nlineplot(ranks, logX = false, logY = false) \",\"replaceOOVs(OOV, baseline.vocab, test.take(10))\",\"injectOOVs(OOV, Seq(\\\"A\\\",\\\"A\\\",\\\"B\\\",\\\"B\\\",\\\"A\\\")) \",\"val train = injectOOVs(OOV, words(trainDocs))\\nval test = replaceOOVs(OOV, train.toSet, words(testDocs))\\nval vocab = train.toSet \\nval baseline = UniformLM(vocab)\\nperplexity(baseline,test) \",\"trait CountLM extends LanguageModel {\\n  def counts:List[String] => Double\\n  def norm:List[String] => Double\\n  def probability(word:String, history:String*) = {\\n    counts(word :: history.toList) / norm(history.toList)  \\n  }\\n}\",\"import scala.collection.mutable.HashMap\\ncase class NGramLM(train:IndexedSeq[String],order:Int) extends CountLM {\\n  val vocab = train.toSet\\n  val counts = new HashMap[List[String],Double] withDefaultValue 0.0\\n  val norm = new HashMap[List[String],Double] withDefaultValue 0.0\\n  for (i <- order until train.length) {\\n    val history = train.slice(i - order + 1, i).toList\\n    val word = train(i)\\n    counts(word :: history) += 1.0\\n    norm(history) += 1.0\\n  }\\n}\",\"val unigram = NGramLM(train,1) \\nbarChart(vocab.map(w => w -> unigram.probability(w)).toSeq.sortBy(-_._2).take(10)) \",\"perplexity(unigram,test) \",\"sample(unigram, Nil, 10).mkString(\\\" \\\") \",\"val bigram = NGramLM(train,2)\\nbarChart(vocab.map(w => w -> bigram.probability(w,\\\"I\\\")).toSeq.sortBy(-_._2).take(10)) \",\"sample(bigram, List(\\\"[BAR]\\\"), 10).mkString(\\\" \\\") \",\"perplexity(bigram,test) \",\"bigram.probability(\\\"[OOV]\\\",\\\"money\\\") \",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => base.counts(h) + alpha\\n  def norm = h => base.norm(h) + alpha * base.vocab.size\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1) \\nlaplaceBigram.probability(\\\"[OOV]\\\",\\\"money\\\")\",\"perplexity(laplaceBigram,test)\",\"case class LaplaceLM(base:CountLM,alpha:Double) extends CountLM {\\n  val eps = 0.001\\n  def vocab = base.vocab\\n  def order = base.order\\n  def counts = h => \\n      (base.counts(h) + alpha) / (base.norm(h.tail) + alpha * base.vocab.size) * (base.norm(h.tail) + eps)\\n  def norm = h => base.norm(h) + eps\\n}\\nval laplaceBigram = LaplaceLM(bigram,0.1)\\nbigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil) -> laplaceBigram.counts(\\\"[OOV]\\\"::\\\"[OOV]\\\":: Nil)\",\"def avgCounts(trainLM:CountLM, testLM:CountLM, vocab:Set[String]) = {\\n  val avgTestCounts = new HashMap[Double,Double] withDefaultValue 0.0\\n  val norm = new HashMap[Double,Double] withDefaultValue 0.0\\n  for (ngram <- cross(List.fill(trainLM.order)(vocab.toList))) {\\n    val trainCount = trainLM.counts(ngram)\\n    val testCount = testLM.counts(ngram)\\n    avgTestCounts(trainCount) += testCount\\n    norm(trainCount) += 1.0\\n  }\\n  for (c <- avgTestCounts.keys) avgTestCounts(c) /= norm(c)\\n  avgTestCounts\\n}\",\"val testBigram = NGramLM(test,2)\\nval jointVocab = (train ++ test).toSet\\nval avgTestCounts = avgCounts(bigram,testBigram,jointVocab)\\nval avgLaplaceCounts = avgCounts(bigram,LaplaceLM(bigram,0.1),jointVocab)\\nval compared = for (i <- 0 until 8) yield Seq(i,avgTestCounts(i),avgLaplaceCounts(i)) \\ntable(Seq(\\\"train-count\\\",\\\"test-count\\\",\\\"smoothed-count\\\") +:compared)\",\"//find bigrams for which the difference between the unigram probability is maximal\\nval differences = for (w1 <- vocab.view if w1 != OOV; w2 <- vocab.view) \\n  yield (w1,w2) -> (unigram.probability(w1) - bigram.probability(w1,w2))\\ndifferences.maxBy(_._2)\",\"case class InterpolatedLM(main:LanguageModel, backoff:LanguageModel, alpha:Double) extends LanguageModel {\\n  def order = main.order\\n  def vocab = main.vocab\\n  def probability(word:String, history:String*) = \\n    alpha * main.probability(word,history:_*) +\\n    (1 - alpha) * backoff.probability(word, history.drop(1):_*)\\n}\",\"val results = for (alpha <- (0 until 20).map(_ / 20.0)) yield \\n  alpha -> perplexity(InterpolatedLM(bigram,unigram,alpha),test)\\nlineplot(results) \",\"case class StupidBackoff(main:CountLM, backoff:LanguageModel, alpha:Double) \\n  extends LanguageModel {\\n    def order = main.order\\n    def vocab = main.vocab\\n    def probability(word:String, history:String*) = \\n      if (main.counts(word :: history.toList) > 0) main.probability(word, history:_*)\\n      else alpha * backoff.probability(word, history.drop(1):_*)\\n      \\n}\"]"
      }
    }
  }, {
    "id" : 61,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThe are several \"proper backoff models\" that do not have this problem, e.g. the Katz-Backoff method. We refer to other material below for a deeper discussion of these.\n\n<div class=\"exercise\">\n  <div class=\"exname\">Exercise 3</div>\n  <div class=\"extext\">Develop and implement a version of the stupid language model that provides probabilities summing up to 1.</div>\n</div>\n\n<div class=\"newslide\"></div>\n### Background Reading\n\n* Jurafsky & Martin, Speech and Language Processing: Chapter 4, N-Grams.\n* Bill MacCartney, Stanford NLP Lunch Tutorial: [Smoothing](http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)",
      "extraFields" : { }
    }
  } ],
  "config" : {
    "autosave" : "false"
  }
}
