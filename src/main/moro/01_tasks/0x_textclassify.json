{
  "name" : "Text Classification",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In many applications we need to automatically classify some input text with respect to a set of classes or labels. For example,\n\n* for information retrieval it is useful to classify documents into a set of topics, such as \"sport\" or \"business\",\n* for sentiment analysis we classify tweets into being \"positive\" or \"negative\" and \n* for Spam Filters we need to distinguish between Ham and Spam.\n\n<!-- TODO: Load Web Corpus, 4 Universities, something were Maxent works -->\n\n### Text Classification as Structured Prediction\nWe can formalize text classification as the simplest instance of [structured prediction](/template/statnlpbook/02_methods/00_structuredprediction) where the input space \\\\(\\Xs\\\\) are sequences of words, and the output space \\\\(\\Ys\\\\) is a set of labels such as \\\\(\\Ys=\\\\{ \\text{sports},\\text{business}\\\\} \\\\). On a high level, our goal is to define a model a model \\\\(s_{\\params}(\\x,y)\\\\) that assigns high *scores* to the label \\\\(y\\\\) that fits the text \\\\(\\x\\\\), and lower scores otherwise. The model will be parametrized by \\\\(\\params\\\\), and these parameters we will learn from some training set \\\\(\\train\\\\) of \\\\((\\x,y)\\\\) pairs. When we need to classify a text \\\\(\\x\\\\) we have to solve the trivial (if the number of classes is low) maximization problem \\\\(\\argmax_y s_{\\params}(\\x,y)\\\\). \n\n<!-- TODO: Show a silly classifier example? --> \n\nIn the following we will present two typical approaches to text classifiers: Naive Bayes and discriminative linear classifiers. We will also see that both in fact can use the same model structure, and differ only in how model parameters are trained.\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Naive Bayes\nOne of the most widely used approaches to text classification relies on the so-called Naive Bayes (NB) Model. In NB we use a distribution \\\\(p^{\\mbox{NB}}_{\\params}\\\\) for \\\\(s_\\params\\\\). In particular, we use the *a posteriori* probability of a label \\\\(y\\\\) given the input text \\\\(\\x\\\\) as a score for that label given the text.   \n\n\\begin{equation}\n  s_{\\params}(\\x,\\y)\\ = p^{\\text{NB}}_{\\params}(y|\\x)\n\\end{equation}\n\nBy Bayes Law we get\n\n\\begin{equation}\n    p^{\\text{NB}}_{\\params}(y|\\x) =\n  \\frac{p^{\\text{NB}}_{\\params}(\\x|y) p^\\text{NB}_{\\params}(y)}{p^{\\text{NB}}_{\\params}(x)}  \n\\end{equation}\n\nand when an input \\\\(\\x\\\\) is fixed we can focus on \n\n\\begin{equation}\\label{eq:NB}\n\\prob^{\\text{NB}}_{\\params}(\\x,y)= p^{\\text{NB}}_{\\params}(\\x|y) p^\\text{NB}_{\\params}(y)\n\\end{equation}\n\nbecause in this case  \\\\(p^{\\text{NB}}_{\\params}(x)\\\\) is a constant factor. In the above \\\\(p^{\\text{NB}}_{\\params}(\\x|y)\\\\) is the *likelihood*, and \\\\(p^\\text{NB}_{\\params}(y) \\\\) is the *prior*.\n\n<!--Let us assume that we have a number \\\\(K(\\x)\\\\) of feature functions \\\\(f_k(\\x)\\\\) that represent the input \\\\(\\x\\\\). For example, in document classification this set could be used to represent the text \\\\(\\x = (x_1,\\ldots,x_n)\\\\) as a bag of words by setting \\\\(f_k(\\x) = x_k\\\\) and \\\\(K(\\x) = n\\\\). We could also use bigrams instead, setting \\\\(f_k(\\x) = (x_k,x_{k+1})\\\\) and \\\\(K(\\x) = n-1\\\\), or any other representation that is effective for distinguishing between classes of text.-->\n\nThe \"naivity\" of NB stems from a certain conditional independence assumption we make for the likelihood \\\\(p^{\\mbox{NB}}_{\\params}(\\x|y)\\\\). Note that conditional independence of two events \\\\(a\\\\) and \\\\(b\\\\) given a third event \\\\(c\\\\) requires that \\\\(p(a,b|c) = p(a|c) p(b|c)\\\\). In particular, for the likelihood in NB we have:\n\n\\begin{equation}\n  p^{\\text{NB}}_{\\params}(\\x|y) = \n  \\prod_i^{\\text{length}(\\x)} p^{\\text{NB}}_{\\params}(x_i|y)\n\\end{equation}\n\nThat is, NB makes the assumption that the observed wors are independent of each other when *conditioned on the label* \\\\(y\\\\). \n\n<!--TODO: Evaluate NB assumption on actual data. Show that features aren't independent, but more independent when conditioned on the label.-->\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n#### Parametrization\nThe NB model has the parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\) where \n\n\\begin{split}\n  p^{\\text{NB}}_{\\params}(f|y) & = \\alpha_{f,y} \\\\\\\\\n  p^{\\text{NB}}_{\\params}(y) & = \\beta_{y}.\n\\end{split}\n\nThat is, \\\\(\\balpha\\\\) captures the per-class feature weights, and \\\\(\\bbeta\\\\) the class priors. \n\n#### Training the Naive Bayes Model\n\nThe NB model again can be trained using Maximum Likelihood estimation. This amounts to setting \n\n\\begin{split}\n  \\alpha_{x,y} & = \\frac{\\counts{\\train}{x,y}}{\\counts{\\train}{y}}\\\\\\\\\n  \\beta_{y} & = \\frac{\\counts{\\train}{y}}{\\left| \\train \\right|}\n\\end{split}\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n#### Log-linear Representation\n\nIt will be convenient to represent the NB model in *log-linear* form. This form will allow us to understand the MLE NB simply as one particular way of training a (log) linear model. It will also make the approach comparable to other approaches such as Conditional Log-Likelihood (aka *logistic regression* or *maximum entropy*) or SVM style training that can operate on the same parametrisation but estimate the parameters using different objectives. Finally, the log-linear representation will make it easy to implement different training algorithms that work with the same representation and hence can be easily plugged-in and out.\n\nIn log-linear form the joint NB distribution \\\\(p^{\\text{NB}}_{\\params}(\\x,y)\\\\) can be written as:  \n\n\\begin{equation}\\label{eq-loglinear} \n  p^{\\text{NB}}_{\\params}(\\x,y)= \\exp \\left( \\sum_{i \\in \\mathcal{I}} f_i(x,y) w_i \\right) = \\exp \\< \\mathbf{f}(\\x,y), \\mathbf{w}\\> \n\\end{equation}\n\nHere the \\\\(f_i\\\\) are so called (joint) *feature functions*. The index set \\\\(\\mathcal{I}\\\\) is the union of all labels \\\\(y'\\in \\Ys\\\\) and all word-label pairs \\\\((x',y')\\\\), and the corresponding feature functions are defined as follows:\n\n\\begin{split}\n  f_{y'}(\\x,y) & = \\delta({y,y'})  \\\\\\\\\n  f_{x',y'}(\\x,y) & = \\delta({y,y'}) \\sum_i^{\\text{length}(\\x)} \\delta(x',x_i) \n\\end{split}\n\nIn words, the first feature function \\\\(f_{y'}\\\\) returns 1 if the input label \\\\(y\\\\) equals \\\\(y'\\\\), and 0 otherwise. The second feature function returns the number of times the word \\\\(x'\\\\) appears in \\\\(\\x\\\\) if \\\\(y\\\\) equals \\\\(y'\\\\), and 0 otherwise.\n\nIf one now sets the weights according to\n\n\\begin{split}\n  w_{y'} & = \\log \\beta_{y'}\\\\\\\\\n  w_{x',y'} & = \\log \\alpha_{x',y'}\n\\end{split}\n\nit is easy to show that \\\\(\\ref{eq-loglinear}\\\\) is equivalent to the original NB formulation in equation \\\\(\\ref{eq:NB}\\\\). \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n#### Feature Templates\nNote that in the above case we have \\\\(|\\Ys|\\\\) label features, and \\\\(|V| \\times |\\Ys|\\\\) word-label features. The corresponding two types of features are often called *feature templates* that generate sets of actual feature functions. That is, \\\\(f_{y'}\\\\) is a feature template with one template *argument*, \\\\(y'\\\\), and \\\\(f_{x'y'}\\\\) is a template with two arguments. It is common to augment the feature templates with a *template name* to distinguish templates that have the same argument space but different semantics. For example, we may have a template that combines the label of a document with number of times word *stems* have been seen. For words \\\\(x'\\\\) that are their own stem this would create duplicate indices, and hence we use \\\\(f_{\\text{word},x',y'}\\\\) and \\\\(f_{\\text{stem},x',y'}\\\\) to distinguish both types of features.\n\n#### Conditional Model\nThe conditional probability \\\\(p_\\params^\\text{NB}(y|\\x)\\\\) can be written as \n\n\\begin{split} \n  p^{\\text{NB}}_{\\params}(y |\\x)= \\frac{\\exp \\< \\mathbf{f}(\\x,y), \\mathbf{w}\\>}{\\sum_{y'} \\exp \\< \\mathbf{f}(\\x,y'), \\mathbf{w}\\>} = \\exp \\left( s_\\params(\\x,y) - A_{\\params,\\x}\\right)\n\\end{split}\n\nwhere \n\n\\begin{split}\n  s_\\params(\\x,y) & = \\< \\mathbf{f}(\\x,y'), \\mathbf{w}\\> \\\\\\\\\n  A_{\\params,\\x} & = \\sum_{y'} \\exp \\< \\mathbf{f}(\\x,y'), \\mathbf{w}\\>\n\\end{split}\n\nand \\\\(A_{\\params,\\x}\\\\) is the *log-partition function*. \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Joint vs Input Features\nIn contrast to the standard formulation of linear and log-linear models we define feature functions over input-output pairs instead of only inputs. That is, we could have also used a representation where \\\\(\\< \\mathbf{f}(\\x,y), \\mathbf{w}\\>\\\\) is replaced by \\\\(\\< \\mathbf{f}(\\x), \\mathbf{w}_y\\>\\\\) in which each class \\\\(y\\\\) receives an own weight vector. \n\nThe benefit of the joint feature function is two-fold. First it enables us to easily define features that break up the output label into sub-labels. That is, say you have the labels \"sports_baseball\" and \"sports_football\", then you can define one feature that tests whether a label starts with a certain prefix (\"sports\"). This allows the model to learn commonalities between both labels. Second, the one-weight vector per class approach breaks down when the output space is structured (and exponentially sized). You simply cannot maintain a different weight vector for each possible output structure \\\\(\\y\\\\), both for computational and statistical reasons.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n### Conditional Loglikelihood \nTraining the NB model using MLE is efficient and easy to implement. In many cases it also leads to good results. However, one can argue that the MLE objective is generally not the optimal choice when the task is to predict the best output given some input. Let us recall the MLE objective: \n\n<!--toDO: (link to Maxent vs NB paper?)-->\n\n\\begin{split}\n  L(\\train, \\params) = \\sum_{(\\x,y) \\in \\train} \\log p^{\\text{NB}}_\\params(\\x,y)\n\\end{split}\n\nVia Bayes Law we can reformulate this objective as follows: \n\n\\begin{split}\n  L(\\train, \\params) = \\sum_{(\\x,y) \\in \\train} \\log p^{\\text{NB}}_\\params(y|\\x) + \\log p^{\\text{NB}}_\\params(\\x) \n\\end{split}\n\nNotice that the \\\\(p^{\\text{NB}}_\\params(\\x) \\\\) here is not the class prior as used in the *forward* definition of the NB model, \\\\(p^{\\text{NB}}_\\params(\\x|y) p^{\\text{NB}}_\\params(y)\\\\). Instead it is the marginal probability \\\\(\\sum_y p^{\\text{NB}}_\\params(\\x,y)\\\\) of seeing a given input text \\\\(\\x\\\\). \n\nThis view on the MLE objective shows that for every training instance MLE means both maximizing the conditional probability of seeing \\\\(y\\\\) given \\\\(\\x\\\\), and maximizing the marginal probability of \\\\(\\x\\\\). Now consider the way we use the NB model to make a prediction for a given \\\\(\\x\\\\): we search for the label \\\\(y\\\\) with maximum conditional probability \\\\(p^{\\text{NB}}_\\params(y|\\x)\\\\). Since \\\\(\\x\\\\) is fixed, the marginal probability \\\\(p^{\\text{NB}}_\\params(\\x)\\\\) has no impact on this decision. This means part of what what we optimize during training is completely irrelevant at test time. \n\nThe above problem may lead to errors: for a given training instance we may increase the marginal probability of \\\\(\\x\\\\) at the price of reducing \\\\(p^{\\text{NB}}_\\params(y|\\x)\\\\) if this means we can still increase their product. This may lead to the true \\\\(y\\\\) having smaller conditional probability than wrong ones.   \n\nOne way to overcome this problem is to directly optimize the *conditional* log-likelihood (CL). This objective is defined as follows:\n\n\\begin{split}\n  CL(\\train, \\params) = \\sum_{(\\x,y) \\in \\train} \\log p^{\\text{NB}}_\\params(y|\\x) = \\sum_{(\\x,y) \\in \\train}  s_\\params(\\x,y) - A_{\\params,\\x} \n\\end{split}\n\nThis objective directly aims to maximize the condional class probability given the inputs, ignoring the marginal probability of \\\\(\\x\\\\).\n",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n#### Optimizing Conditional Loglikelihood \nDue to the normalization factor \\\\(A_{\\params,\\x}\\\\) there is no closed-form solution to the CL problem, and compared to the MLE solution we cannot just count. Instead we need to iteratively optimize the objective. One popular method to optimize the CL objective is via gradient ascent, or its stochastic version, stochastic gradient ascent Often one uses the term gradient *descent* even when ascent is meant, and we then speak of stochastic gradient descent (SGD). \n\nThe underlying idea of such gradient-based methods is to iteratively move along the gradient of the function until this gradient disappears. That is, for a function \\\\(f(\\params)\\\\) to be optimized, in each step \\\\(i\\\\) a gradient ascent algorithm performs the following update to the parameter vector \\\\(\\params\\\\):\n\n\\begin{split}\n  \\params_i \\leftarrow \\params_{i-1} + \\alpha_i \\nabla f(\\params_{i-1}) \n\\end{split}\n\nThere are various ways of choosing the *learning rate* \\\\(\\alpha\\\\) dynamically depending on the iteration \\\\(i\\\\), but for simplicity we will set it to 1 in the following.\n\nTo apply gradient ascent methods to optimizing the conditional loglikelihood we need to find its gradient with respect to the parameters. This gradient has a very intuitive form:\n\n\n\\begin{split}\n  \\nabla CL(\\params) =  \\sum_{(\\x,y) \\in \\train} \\mathbf{f}(\\x,y) - E_{p_\\params(y'|\\x)}\\[ \\mathbf{f}(\\x,y')\\]\n\\end{split}\n\nThat is, for each training instance this gradient moves towards the feature representation \\\\(\\mathbf{f}(\\x,y)\\\\) of the gold solution, and away from the expectation of the feature representation under the current model \\\\(E_{p_\\params(y'|\\x)}\\[ \\mathbf{f}(\\x,y')\\]\\\\). \n\nIn particular, this gradient is zero (and hence at an optimal solution) when the empirical expectation of the feature function under the training set is identical to the (conditional) model expectation. This gives rise to a dual view of the conditional likelihood objective: we can see solutions to this problem as parameters that force the empirical and model moments to match. In fact, one can arrive at the log-linear formulation and the CL objective also by searching for any distribution that matches the given feature moments and has maximal entropy. Particularly in the context of text classification the CL based approach is therefore often referred to as *Maximum Entropy* approach. On many text classification datasets it can be shown to outperform the MLE approach substantially.    \n\nNote that the CL objective is strictly concave. This means that when the gradient becomes zero we have found the single optimum to this function. \n<!--#### -->\n\n<!--#### Stochastic Gradient Descent-->\n<!--There are various gradient-based algorithms to find this optimum, as well as higher order methods that use the functions Hessian or approximations thereof. However, in this book we will focus on the most simple approach and generally use gradient ascent with fixed step size.-->\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\n\n \n \n ",
      "extraFields" : { }
    }
  } ],
  "config" : { }
}
