{
  "name" : "Word-based Machine Translation",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nMachine Translation (MT) is one of the canonical NLP applications, and one that nowadays most people are familiar with, primarily through online translation services of the major search engine providers. While there is still some way to go before machines can provide fluent and flawless translations, in particular for more distant language pairs like English and Japanese, progress in this field has been remarkable. <span class=\"summary\">MT is one of the most widely used NLP application. It is an example of **end-to-end NLP**, and MT-like architectures can be found in many other applications.</span> \n\n<div class=\"newslide\"></div>\nIn this chapter we will illustrate the foundations of this progress, and focus on word-based machine translation models. In such models words are the basic unit of translation. Nowaways the field has mostly moved to phrase and syntax-based approaches, but the word-based approach is still important, both from a foundational point of view, and as sub-component in more complex approaches. <span class=\"summary\">We focus on **word-based** MT which is not the state-of-the-art anymore, but a **foundation and blueprint** for modern mechanisms.</span>   \n\n<div class=\"newslide\"></div>\n### MT as [Structured Prediction](/template/statnlpbook/02_methods/00_structuredprediction)\n\nFormally we will see MT as the task of translating a _source_ sentence \\\\(\\source\\\\) to a _target_ sentence \\\\(\\target\\\\). We can tackle the problem using the [structured prediction recipe](/template/statnlpbook/02_methods/00_structuredprediction): We define a parametrised model \\\\(s_\\params(\\target,\\source)\\\\) that measures how well a target  \\\\(\\target\\\\) sentence matches a source sentence \\\\(\\source\\\\), learn the parameters \\\\(\\params\\\\) from training data, and then find <span class=\"summary\">Define a parametrised model \\\\(s_\\params(\\target,\\source)\\\\) measuring how well a **target** sentence \\\\(\\target\\\\)  matches a **source** sentence \\\\(\\source\\\\), learn the parameters \\\\(\\params\\\\) from training data, and find</span> \n\n\\begin{equation} \n  \\label{decode-mt}\n  \\argmax_\\target s_\\params(\\target,\\source) \n\\end{equation}\n\nas translation of \\\\(\\source\\\\). Different _statistical_ MT approaches, in this view, differ primarily in how \\\\(s\\\\) is defined, \\\\(\\params\\\\) are learned, and how the \\\\(\\argmax\\\\) is found. <span class=\"summary\"></span>\n",
      "extraFields" : { }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\n### Noisy Channel Model for MT\n\n<div class=\"book-start\"></div>\n\nMany Word-based MT systems, as well as those based on more advanced representations, rely on a [Noisy Channel](https://www.dropbox.com/s/gfucv538m6anmgd/NoisyChannel.pdf?dl=0) model as choice for the scoring function \\\\(s_\\params\\\\). In this approach to MT we effectively model the translation process *in reverse*. That is, we assume that a probabilistic process (the speaker's brain) first generates the target sentence \\\\(\\target\\\\) according to the distribution \\\\(\\prob(\\target)\\\\). Then the target sentence \\\\(\\target\\\\) is transmitted through a _noisy channel_ \\\\(\\prob(\\source|\\target)\\\\) that translates \\\\(\\target\\\\) into \\\\(\\source\\\\). \n\n<div class=\"slide-start\"></div>\n\nA generative process (the speaker's brain): \n\n* generate target sentence \\\\(\\target\\\\) according to \\\\(\\prob(\\target)\\\\). \n* Transmit \\\\(\\target\\\\) through a _noisy channel_ \\\\(\\prob(\\source|\\target)\\\\) to generate source \\\\(\\source\\\\).\n\nFor a graphical view on the noisy channel see my [slides](https://www.dropbox.com/s/gfucv538m6anmgd/NoisyChannel.pdf?dl=0). \n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\nHence translation is seen as adding noise to a clean \\\\(\\target\\\\). This _generative story_ defines a _joint distribution_ over target and source sentences \\\\(\\prob(\\source,\\target) = \\prob(\\target) \\prob(\\source|\\target) \\\\). We can in turn operate this distribution in the direction we actually care about: to infer a target sentence \\\\(\\target\\\\) given a source sentence \\\\(\\source\\\\) we find the _maximum a posteriori_ sentence <span class=\"summary\">The distribution over target words and the channel noise model define a joint distribution \\\\(\\prob(\\source,\\target) = \\prob(\\target) \\prob(\\source|\\target) \\\\). To use it for translation we operate it **backwards**: </span>\n\n\\begin{equation}\n\\label{decode-nc}\n\\target^* = \\argmax_\\target \\prob(\\target | \\source) = \\argmax_\\target \\prob(\\target) \\, \\prob(\\source | \\target). \n\\end{equation}\n\nFor the structured prediction recipe this means setting \n\n$$\ns_\\params(\\target,\\source) = \\prob(\\target) \\, \\prob(\\source | \\target). \n$$\n\n<div class=\"newslide\"></div>\n<div class=\"book-start\"></div>\nIn the noisy channel approach for MT the distribution \\\\(\\prob(\\target)\\\\) that generates the target sentence is usually referred to as [language model](/template/statnlpbook/01_tasks/01_languagemodels), and the noisy channel is called the _translation model_. As we have discussed language models earlier, in this chapter we focus on the translation model \\\\(\\prob(\\source|\\target)\\\\).\n<div class=\"slide-start\"></div>\n\nIn Machine Translation:\n\n* \\\\(\\prob(\\target)\\\\) is usually referred to as [language model](/template/statnlpbook/01_tasks/01_languagemodels).\n* \\\\(\\prob(\\source|\\target)\\\\) is referred to as **translation model**.\n\nHere we focus on translation models. \n\n<div class=\"slide-end\"></div>\n\n\n<div class=\"newslide\"></div>\n### A Naive Baseline Translation Model\nThe most straightforward translation model translates words one-by-one, in the order of appearance:\n$$\n\\prob_\\params^\\text{Naive}(\\ssource|\\starget) = \\prod_i^{\\length{\\source}} \\param_{\\ssource_i,\\starget_i}\n$$\nwhere \\\\(\\param_{\\ssource,\\starget} \\\\) is the probability of translating \\\\(\\starget\\\\) as \\\\(\\ssource\\\\). \\\\(\\params\\\\) is often referred to as *translation table*.\n\n<div class=\"newslide\"></div>\nFor many language pairs one can acquire training sets \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\) of paired source and target sentences. For example, for French and English the [Aligned Hansards](http://www.isi.edu/natural-language/download/hansard/) of the Parliament of Canada can be used. Given such a training set \\\\(\\train\\\\) we can learn the parameters \\\\(\\params\\\\) using the [Maximum Likelhood estimator](/template/statnlpbook/02_methods/0x_mle). In the case of our Naive model this amounts to setting <span class=\"summary\">Using **parallel** data of paired source and target sentences, we can train the model using the **Maximum Likelihood Estimator**:</span>\n\n$$\n\\param_{\\ssource,\\starget} = \\frac{\\counts{\\train}{s,t}}{\\counts{\\train}{t}} \n$$\n\nHere \\\\(\\counts{\\train}{s,t}\\\\) is the number of times we see target word \\\\(t\\\\) translated as source word \\\\(s\\\\), and \\\\(\\counts{\\train}{t}\\\\) the number of times we the target word \\\\(t\\\\) in total.\n\n<!--\n<div class=\"exercise\">\n  <div class=\"exname\">Exercise</div>\n  <div class=\"extext\">Derive the maximum likelhood estimate for \\\\(\\prob_\\params^\\text{Naive}\\\\) for training data \\\\(\\train=\\left( \\left(\\source_i,\\target_i\\right) \\right)_{i=1}^n \\\\). </div>\n</div>\n-->\n\n\n<div class=\"newslide\"></div>\n#### Training the Naive Model\nLet us preprare some toy data to show how train this naive model.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val train = Seq(\n  \"the house is small\" -> \"das Haus ist klein\",\n  \"the house is small\" -> \"klein ist das Haus\",\n  \"a man is tall\" -> \"ein Mann ist groß\",\n  \"my house is small\" -> \"klein ist mein Haus\"\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\ntrain.size ",
      "extraFields" : {
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nNotice how we transformed raw strings into Document objects via `segment`, and how we then fill the training set with Sentence objects by extracting the documents `head` sentence from each document. This dataset can be used to train the naive model as follows. <span class=\"summary\">We can train the naive model as follows.</span>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import scala.collection.mutable.HashMap\ncase class Param(src:String, tgt: String, prob: Double)\ntype NaiveModel = Seq[Param]\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\n  //data structures to store counts in nominator and denominator\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\n  //do the counting\n  for ((target,source) <- data) {\n    for (i <- 0 until target.tokens.length) {\n      norm(target.tokens(i).word) += 1.0\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\n    }    \n  }\n  //convert the map to a sequence of parameters\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\n} ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us train on the toy dataset:",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val model = learn(train)\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \"is\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \"]"
      }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<!--\n<div class=\"newslide\"></div>\n##### Exercise\nImplement an non-procedural [functional](https://www.coursera.org/course/progfun) version of this code.\n\n##### Exercise\nMake this work even when sentences don't have the same length. \n\n##### Exercise\nWhich sentences make the problem easier, which harder?\n-->\n\n<div class=\"newslide\"></div>\n#### Decoding with the Naive Model\n\n<div class=\"book-start\"></div>\n\n*Decoding* in MT is the task of finding the solution to equation \\\\(\\ref{decode-mt}\\\\). That is, we need to find that target sentence with maximum a posteriori probability, which is equivalent to finding the target sentence with maximum likelihood as per equation \\\\(\\ref{decode-nc}\\\\). The phrase \"decoding\" relates to the noisy channel analogy. Somebody generated a message, the channel encodes (translates) this message and the receiver needs to find out what the original message was.   \n\nIn the naive model decoding is trivial if we assume a unigram language model. We need to choose, for each source word, the target word with maximal product of translation and langugage model probability. For more complex models this is not sufficient, and we discuss a more powerful decoding method later.\n\n<div class=\"slide-start\"></div>\nTranslation is often called **decoding** and means:\n$$\n  \\argmax_\\target \\prob(\\target) \\, \\prob(\\source | \\target)\n$$\nHow can we achieve this for the Naive Model?\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\n\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\n  //create a mapping from source words to relevant parameters\n  val source2targets = model groupBy (_.src) \n  def translate(token:Token) = {\n    val candidates:Seq[Param] = source2targets(token.word)\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\n    scoredByLM.maxBy(_.prob).tgt\n  }\n  //translate word-by-word (for which LM does this work?)\n  val words = source.tokens map translate\n  Document(IndexedSeq(words))\n}\nval source = train(1)._2 //\"klein ist das Haus\"\nval lm = UniformLM(model.map(_.tgt).toSet)\nval target = decode(source, model, lm) \ntarget.toText ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \"]"
      }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThe naive model is broken in several ways. Most severly, it ignores the fact that word order can differ and still yield (roughly) the same meaning.   \n\n<div class=\"newslide\"></div>\n### IBM Model 2\nThe IBM Model 2 is one of the most influential translation models, even though these days it is only indirectly used in actual MT systems, for example to initialize translation and alignment models. As IBM Model 2 can be understood as generalization of IBM Model 1, we omit the latter for now and briefly illustrate it afterward our introduction of Model 2. Notice that parts of these exposition are based on the excellent [lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) of Mike Collins. <span class=\"summary\">IBM Model 2 is one of the most influential translation models, even though these days it is only indirectly used in actual MT systems, for example to initialize translation and alignment models.</span> \n\n<div class=\"newslide\"></div>\n#### Alignment\n<div class=\"book-start\"></div>\n\nThe core difference of Model 2 to our naive baseline model is the introduction of _latent_ auxiliary variables: the word to word _alignment_ \\\\(\\aligns\\\\) between words. In particular, we introduce a variable \\\\(a_i \\in [0 \\ldots \\length{\\target}]\\\\) for each source sentence index \\\\(i \\in [1 \\ldots \\length{\\source}]\\\\). The word alignment \\\\(a_i = j \\\\) means that the source word at token \\\\(i\\\\) is _aligned_ with the target word at index \\\\(j\\\\). \n\n\nNotice that \\\\(\\align_i\\\\) can be \\\\(0\\\\). This corresponds to a imaginary _NULL_ token \\\\(\\starget_0\\\\) in the target sentence and allows source words to be omitted in an aligment. \n\n<div class=\"slide-start\"></div>\n\nThe core difference of Model 2 is the introduction of _latent_ auxiliary variables \\\\(\\aligns\\\\):\n\n* \\\\(a_i \\in [0 \\ldots \\length{\\target}]\\\\) for each source token \\\\(i \\in [1 \\ldots \\length{\\source}]\\\\)\n* \\\\(a_i = j \\\\) means source token \\\\(i\\\\) is _aligned_ with the target token \\\\(j\\\\).\n* \\\\(\\align_i\\\\) can be \\\\(0\\\\) to point to a _NULL_ token to omit source words. \n\n<div class=\"slide-end\"></div>\n\n\n\n<!--\n<p class=\"exercise\">\nHow can _target_ words be omitted in an alignment?\n</p>\n-->\n\n<div class=\"newslide\"></div>\nBelow you see a simple example of an alignment.\n\n\n\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val target = segment(\"NULL the house is small\").sentences(0)\nval source = segment(\"klein ist das Haus\").sentences(0)\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \"]"
      }
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val target2 = segment(\"NULL 家 わ 小さいい です\").sentences(0)\nval source2 = segment(\"The house is small\").sentences(0)\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\"]"
      }
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n\n<div class=\"newslide\"></div>\nIBM Model 2 defines a conditional distribution \\\\(\\prob(\\source,\\aligns|\\target)\\\\) over both the source sentence \\\\(\\source\\\\) and its alignment \\\\(\\aligns\\\\) to the target sentence \\\\(\\target\\\\). Such a model can be used as translation model \\\\(\\prob(\\source|\\target)\\\\), as defined above, by marginalizing out the alignment <span class=\"summary\"> Model 2 defines a distribution \\\\(\\prob(\\source,\\aligns|\\target)\\\\). A **translation model** can be derived by marginalizing out the alignments</span>\n\n$$\n\\prob(\\source|\\target) = \\sum_{\\aligns} \\prob(\\source,\\aligns|\\target).\n$$\n\n<!--\n<div class=\"newslide\"></div>\n\nNotice that pluging this translation model into the argmax problem of equation \\\\(\\ref{decode-nc}\\\\) results in a nested summation and maximization problem. [Park and Darwiche](http://arxiv.org/pdf/1107.0024.pdf) show that this problem is generally substantially harder than only performing the max. To avoid the computational issues that arise in this setting, it is therefore common to instead maximize over both target and alignment at decoding time: <span class=\"summary\">For computational reasons we often search over both alignments and targets</span>\n\n$$\n(\\target^*,\\aligns^*) = \\argmax_{\\target,\\aligns} \\prob(\\target, \\aligns | \\source).\n$$\n\nNotice that marginalization happens to be feasible for Model 2, but generally latent alignments models do not have this property. \n\n-->\n\n\n<div class=\"newslide\"></div>\n#### Model Parametrization\n\n<div class=\"book-start\"></div>\n\nIBM Model 2 defines its conditional distribution over source and alignments using two sets of parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\). Here \\\\(\\alpha(\\ssource|\\starget)\\\\) is a parameter defining the probability of translation target word \\\\(\\starget\\\\) into source word \\\\(\\ssource\\\\), and \\\\(\\beta(j|i,l_\\starget,l_\\ssource)\\\\) a parameter that defines the probability of aligning the source word at token \\\\(i\\\\) with the target word at token \\\\(j\\\\), conditioned on the length \\\\(l_\\starget\\\\) of the target sentence, and the length \\\\(l_\\ssource\\\\) of the source sentence. \n\n<div class=\"slide-start\"></div>\n\nIBM Model 2 has two sets of parameters \\\\(\\params=(\\balpha,\\bbeta)\\\\)\n\n* \\\\(\\alpha(\\ssource|\\starget)\\\\): probability of translation target word \\\\(\\starget\\\\) into source word \\\\(\\ssource\\\\). \n* \\\\(\\beta(j|i,l_\\starget,l_\\ssource)\\\\): probability of aligning the source token \\\\(i\\\\) with target token \\\\(j\\\\), conditioned on the target length \\\\(l_\\starget\\\\) and source length \\\\(l_\\ssource\\\\).  \n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\nWith the above parameters, IBM Model 2 defines a conditional distribution over source sentences and alignments, conditioned on a target sentence _and a desired source sentence length_ \\\\(l_\\ssource\\\\): <span class=\"summary\">Model 2 defines a conditional distribution over source sentences and alignments, conditioned on a target sentence _and a desired source sentence length_</span>\n\n\\begin{equation}\n\\label{ibm2}\n  p_\\params^\\text{IBM2}(\\ssource_1 \\ldots \\ssource_{l_\\ssource},\\align_1 \\ldots \\align_{l_\\ssource}|\\starget_1 \\ldots \\starget_{l_\\starget}, l_\\ssource) = \\prod_i^{l_\\ssource} \\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)\n\\end{equation}\n",
      "extraFields" : {
        "aggregatedCells" : "[\"1 + 3\\n\"]"
      }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\n### Training IBM Model 2 with the EM Algorithm\n\n<div class=\"book-start\"></div>\n\nTraining IBM Model 2 is less straightforward than training our naive baseline. The main reason is the lack of _gold alignments_ in the training data. That is, while we can quite easily find, or heuristically construct, _sentence-aligned_ corpora like our toy dataset, we generally do not have _word aligned_ sentences.\n\n<div class=\"slide-start\"></div>\nTraining Model 2 is challenging:\n\n  * we have gold sentence alignments \n  * but **no gold word alignments**!\n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\nTo overcome this problem, IBM Model can be trained using the Expectation Maximization (EM) Algorithm, a general recipe when learning with partially observed data&mdash;in our case the data is partially observed because we observe the source and target sentences, but not their alignments. The EM algorithm maximizes a lower bound bound of the log-likelihood of the data. The log-likelihood of the data is <span class=\"summary\">The Expectation Maximization (EM) Algorithm is a standard training algorithm for **partially observed data**. It maximizes a lower bound of the likelihood</span>:\n\n$$\n  \\sum_{(\\target_i,\\source_i) \\in \\train} \\log p_\\params^\\text{IBM2}(\\source_i|\\target_i) =  \\sum_{(\\target_i,\\source_i) \\in \\train} \\log \\sum_{\\aligns}  p_\\params^\\text{IBM2}(\\source_i,\\aligns|\\target_i) \n$$\n\nEM can be be seen as [block coordinate descent](https://www.dropbox.com/s/vrsefe3m57bxpgv/EMforTM.pdf?dl=0) on this bound.\n\n<div class=\"newslide\"></div>\n\nThe EM algorithm is an iterative method that iterates between two steps, the E-step (Expectation) and the M-Step (Maximization), until convergence. For the case of IBM Model 2 the E and M steps are instantiated as follows: <span class=\"summary\">EM for Model 2 iterates between:</span> \n\n  * **E-Step**: given a current set of parameters \\\\(\\params\\\\), calculate the **expectations** \\\\(\\pi\\\\) of the latent alignment variables under the model \\\\(p_\\params^\\text{IBM2}\\\\) &mdash; this amounts to estimating a _soft alignment_ for each sentence.    \n  * **M-Step**: Given training set of soft alignments \\\\(\\pi\\\\), find new parameters \\\\(\\params\\\\) that **maximize** the log likelihood of this (weighted) training set. This amounts to soft counting. \n\n<div class=\"newslide\"></div>\n#### E-Step\n<div class=\"book-start\"></div>\n\nThe E-Step calculates the distribution\n\n$$\n\\pi(\\aligns|\\source,\\target) = p_\\params^\\text{IBM2}(\\aligns|\\source,\\target)\n$$\n\nfor the current parameters \\\\(\\params\\\\). For Model 2 this distribution has a very simple form:\n\n$$\n\\pi(\\aligns|\\source,\\target) = \\prod_i^{l_{\\ssource}} \\pi(a_i|\\source,\\target,i) = \\prod_i^{l_{\\ssource}} \n  \\frac\n    {\\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)}\n    {\\sum_j^{l_{\\starget}} \\alpha(\\ssource_i|\\starget_j) \\beta(j|i,l_\\starget,l_\\ssource) }\n$$\n\nImportantly, the distribution over alignments *factorizes* in a per-source-token fashion, and hence we only need to calculate, for each source token \\\\(i\\\\) and each possible alignment \\\\(a_i\\\\), the probability (or expectation) \\\\(\\pi(a_i|\\source,\\target,i)\\\\).\n\n<div class=\"slide-start\"></div>\n\n* Calculate distribution over alignments \n\n$$\n\\pi(\\aligns|\\source,\\target) = p_\\params^\\text{IBM2}(\\aligns|\\source,\\target)\n$$\n\n* Distribution **factorizes**:\n\n$$\n\\pi(\\aligns|\\source,\\target) = \\prod_i^{l_{\\ssource}} \\pi(a_i|\\source,\\target,i) = \\prod_i^{l_{\\ssource}} \n  \\frac\n    {\\alpha(\\ssource_i|\\starget_{a_i}) \\beta(a_i|i,l_\\starget,l_\\ssource)}\n    {\\sum_j^{l_{\\starget}} \\alpha(\\ssource_i|\\starget_j) \\beta(j|i,l_\\starget,l_\\ssource) }\n$$\n\n\n<div class=\"slide-end\"></div>\n\n\n<div class=\"newslide\"></div>\nBefore we look at the implementation of this algorithm we will set up the training data to be compatible with our formulation. This involves introducing a 'NULL' token to each target sentence to allow source tokens to remain unalligned. We also gather a few statistics that will be useful in our implementation later on. <span class=\"summary\">First we create some toy data:</span>  \n",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val trainM2 = Seq(\n  \"NULL the house is small\" -> \"klein ist das Haus\",\n  \"NULL a man is tall\" -> \"groß ist ein Mann\",\n  \"NULL my house is small\" -> \"klein ist mein Haus\",\n  \"NULL the building is big\" -> \"groß ist das Gebäude\",\n  \"NULL the building is long\" -> \"lang ist das Gebäude\"\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nWe can now implement the E-Step.",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \"]"
      }
    }
  }, {
    "id" : 16,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "//Each element in the sequence corresponds to one source token,\n//and each element in the nested sequence to a target token.\ntype Align = IndexedSeq[IndexedSeq[Double]]\n\n//Model parameters\ncase class Model(\n  alpha:Map[(String,String),Double],\n  beta:Map[(Int,Int,Int,Int),Double]) \n\n//normalizing a distribution\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \n\n//estimate new soft alignments\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\n  for ((target,source) <- data) yield {\n    def score(si:Int, ti:Int) = \n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \n      model.beta(ti,si,target.tokens.length, source.tokens.length)\n    for (si <- source.tokens.indices) yield \n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\n  }\n}",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us run this code:",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \nval init = Model(\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\n\nval alignments = eStep(init,trainM2) \nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\"]"
      }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "You can play around with the initialization of \\\\(\\bbeta\\\\) to see how the alignments react to changes of the word-to-word translation probabilities.\n\n<div class=\"newslide\"></div>\n#### M-Step\n\n<div class=\"book-start\"></div>\n\nThe M-Step optimizes a *weighted* or *expected* version of the log-likelihood of the data, using the distribution \\\\(\\pi\\\\) from the last E-Step:\n\n$$\n  \\params^* = \\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\sum_\\aligns \\pi(\\aligns|\\target,\\source) \\log \\prob _\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n$$\n\nThe summing over hidden alignments seems daunting, but because \\\\(\\pi\\\\) factorizes as we discussed above, we again have a simple closed-form solution:\n\n$$\n  \\alpha(\\ssource|\\starget) = \\frac\n    {\\sum_{(\\target,\\source)}\\sum_i^{l_\\source} \\sum_j^{l_\\target} \\pi(j|i) \\delta(\\ssource,\\ssource_i) \\delta(\\starget,\\starget_j) }\n    {\\sum_{(\\target,\\source)} \\sum_j^{l_\\target} \\delta(\\starget,\\starget_j) }\n$$\n\nwhere \\\\(\\delta(x,y)\\\\) is 1 if \\\\(x=y\\\\) and 0 otherwise. The updates for \\\\(\\beta\\\\) are similar. \n\n<div class=\"slide-start\"></div>\n\n* Calculate **weighted** maximum likelihood estimate:\n\n$$\n  \\params^* = \\argmax_\\params \\sum_{(\\target,\\source) \\in \\train} \\sum_\\aligns \\pi(\\aligns|\\target,\\source) \\log \\prob _\\params^\\text{IBM2}(\\source,\\aligns|\\target)\n$$ \n\n* Closed form solution (weighted counting):\n\n$$\n  \\alpha(\\ssource|\\starget) = \\frac\n    {\\sum_{(\\target,\\source)}\\sum_i^{l_\\source} \\sum_j^{l_\\target} \\pi(j|i) \\delta(\\ssource,\\ssource_i) \\delta(\\starget,\\starget_j) }\n    {\\sum_{(\\target,\\source)} \\sum_j^{l_\\target} \\delta(\\starget,\\starget_j) }\n$$\n\n\\\\(\\delta(x,y)\\\\) is 1 if \\\\(x=y\\\\) and 0 otherwise\n\n<div class=\"slide-end\"></div>\n\n<div class=\"newslide\"></div>\nLet us implement the M-Step now. In this step we estimate parameters \\\\(\\params\\\\) from a given set of (soft) alignments \\\\(\\aligns\\\\). \n\n",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map(p => (p.src,p.tgt) -> p.prob))\",\"def decode(source:Sentence, model:Model) = {\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy (_.prob)).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval target = decode(source, model)\\nrenderTokens(target)\",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"def eStep() = {\\n  1\\n}\\neStep()\"]"
      }
    }
  }, {
    "id" : 20,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\n  for ((pi,(t,s)) <- aligns zip data) {\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\n    }  \n  }                 \n  Model(\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \n}\n",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us run one M-Step on the alignments we estimated earlier.",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val theta1 = mStep(alignments,trainM2)\nbarChart(theta1.alpha.filter(_._1._2 == \"is\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\"]"
      }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that the algorithm already figured out that \"is\" is most likely translated to \"ist\". This is because it is (softly) aligned with \"is\" in every sentence, whereas other German words only appear in a subset of the sentences.",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "\n<div class=\"newslide\"></div>\n#### Initialization (IBM Model 1)\nWe could already iteratively call `eStep` and `mStep`until convergence. However, a crucial question is how to initialize the model parameters for the first call to 'eStep'. So far we used a uniform initialization, but given that the EM algorithm's results usually depend significantly on initialization, using a more informed starting point can be useful. <span class=\"summary\">Due to non-convexity of the EM bound, good initialization is crucial. </span>\n\n<div class=\"newslide\"></div>\nA common way to initialize EM for IBM Model 2 training is to first train the so called IBM Model 1 using EM. This model really is an instantiation of Model 2 with a specific and **fixed** alignment parameter set \\\\(\\bbeta\\\\). Instead of estimating \\\\(\\bbeta\\\\) it is set to assign uniform probability to all target tokens with respect to a given length: <span class=\"summary\">We can train **IBM Model 1**, which fixes the distortion parameters \\\\(\\bbeta\\\\) to be uniform:</span>\n\n$$\n  \\beta(a_i|i,l_\\starget,l_\\ssource) = \\frac{1}{l_\\starget + 1}\n$$\n\nAfter training the parameters \\\\(\\params\\\\) of Model 1 can be used to initialize EM for Model 2.  \n\n<div class=\"newslide\"></div>\nTraining Model 1 using EM could have the same initialization problem. Fortunately it turns out that with \\\\(\\bbeta\\\\) fixed in this way it can be shown, under mild conditions, that EM will converge to a global optimum, making IBM Model 1 robust to choices of initialization. <span class=\"summary\">EM for IBM Model 1 converges to a global optimum.</span>\n\n<div class=\"newslide\"></div>\nLet us train IBM Model 1 now. This amounts to using our previous `eStep` and `mStep` methods, initializing \\\\(\\bbeta\\\\) as above and not updating it during `mStep`. ",
      "extraFields" : { }
    }
  }, {
    "id" : 25,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def change(a1:Seq[Align],a2:Seq[Align]) = {\n  val flatA1 = a1.flatMap(_.flatten) \n  val flatA2 = a2.flatMap(_.flatten)\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\n  diffs.sum / flatA1.length\n}\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\n  var model = init\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\n  for (_ <- 0 until iterations) yield {\n    val old = alignments\n    alignments = eStep(model, data)\n    model = mStep(alignments, data).copy(beta = init.beta) \n    (alignments,model,change(old,alignments))\n  }                   \n} ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 26,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nYou can see below that the alignments converge relatively quickly.   ",
      "extraFields" : { }
    }
  }, {
    "id" : 27,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val ibm1Iterations = emModel1(init, trainM2, 100)\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \nlineplot(xyData)  ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \"]"
      }
    }
  }, {
    "id" : 28,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n<div class=\"start-book\"></div>\n<div class=\"exercise\">\n  <div class=\"exname\">Exercise 1</div>\n  <div class=\"extext\">Can you think of other reasonable measures for convergence? Hint: consider the formal derivation of EM. </div>\n</div>\n<div class=\"start-slide\"></div>\n<div class=\"end-slide\"></div>\n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us have a look at the translation table.",
      "extraFields" : { }
    }
  }, {
    "id" : 30,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \"house\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \"]"
      }
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div> \nWe can also inspect the alignments generated during EM.",
      "extraFields" : { }
    }
  }, {
    "id" : 32,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \"]"
      }
    }
  }, {
    "id" : 33,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Training IBM Model 2\nNow that we have a reasonable initial model we can use it to initialize EM for IBM Model 2. Here is the EM code in full.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 34,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\n  var model = init\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\n  for (_ <- 0 until iterations) yield {\n    val old = alignments\n    alignments = eStep(model, data)\n    model = mStep(alignments, data)\n    (alignments,model,change(old,alignments))\n  }                   \n} ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 35,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nInitializing with the IBM Model 1 result gives us: ",
      "extraFields" : { }
    }
  }, {
    "id" : 36,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val ibm1 = ibm1Iterations.last._2\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\nval ibm2 = ibm2Iterations.last._2\nbarChart(ibm2.alpha.filter(_._1._2 == \"house\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \"]"
      }
    }
  }, {
    "id" : 37,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nFor alignments we get:\n",
      "extraFields" : { }
    }
  }, {
    "id" : 38,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val ibm2Alignments = ibm2Iterations.last._1\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \"]"
      }
    }
  }, {
    "id" : 39,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us look at the distortion probabilities for a given source position and source and target lengths.",
      "extraFields" : { }
    }
  }, {
    "id" : 40,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2.beta(ti,si,5,4) \nbarChart(distort(0), color = Some(\"#fc0\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \",\"val ibm2Alignments = ibm2Iterations.last._1\\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   \"]"
      }
    }
  }, {
    "id" : 41,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\n#### Decoding for IBM Model 2\n\nDecoding IBM Model 2 requires us to  solve the argmax problem in equation \\\\(\\ref{decode-nc}\\\\), this time using the conditional probability from equation \\\\(\\ref{ibm2}\\\\) with the hidden alignments marginalized out: <span class=\"summary\">Decoding IBM Model 2 ideally means:</span>\n\n\\begin{equation}\n  \\argmax_{\\target} p_\\params^\\text{IBM2}(\\source | \\target) = \n  \\argmax_{\\target} \\sum_{\\aligns} p_\\params^\\text{IBM2}(\\source,\\aligns | \\target)\n\\end{equation}\n\n<div class=\"newslide\"></div>\nThis nested argmax and sum is generally computationally very hard (see [Park and Darwiche](http://arxiv.org/pdf/1107.0024.pdf)), and often replaced with the simpler problem of finding a combination of best target sequence and corresponding alignment. <span class=\"summary\">This nested argmax/sum is generally **computationally very hard**. It's easier to argmax over target and alignments: </span>   \n\n\\begin{equation}\n  \\argmax_{\\target,\\aligns} p_\\params^\\text{IBM2}(\\source,\\aligns | \\target)\n\\end{equation}\n\nAs it turns out for IBM Model 2 the sum can be efficiently calculated, and [Wang and Waibel](http://aclweb.org/anthology/P/P97/P97-1047.pdf) show a stack based decoder that does take this into account. <span class=\"summary\"></span>   \n\n<div class=\"newslide\"></div>\nHowever, both for simplicity of exposition and because for most real-world models this marginalization is not possible, we present a decoder that searches over both target and alignment. To simplify the algorithm further we assume that target and source sentences have to have them same length. Of course this is a major restriction, and it is not necessary, but makes the algorithm easier to explain while maintaining the core mechanism. Here we only show only the Scala code and refer the reader to our [slides](https://www.dropbox.com/s/p495n19h5rtk3uf/IBM-decoding.pdf?dl=0) for an illustration of how stack and beam based decoders work. <span class=\"summary\">A simplified **same-length** decoder in Scala, using a [beam](https://www.dropbox.com/s/p495n19h5rtk3uf/IBM-decoding.pdf?dl=0):</span>  ",
      "extraFields" : { }
    }
  }, {
    "id" : 42,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "case class Hypothesis(target:List[String], align:Map[Int,Int], remaining:Set[Int],\n                      score:Double, parent:Option[Hypothesis]= None)\n\ntype Beam = List[Hypothesis]\n \ndef decodeModel2(tm:Model, lm:LanguageModel, \n                 source:IndexedSeq[String], beamSize:Int) = {\n  \n  def score(hyp:Hypothesis,newTarget:String,sourceIndex:Int) = {\n    val targetLength = source.length + 1\n    val history = hyp.target.take(lm.order - 1).reverse\n    val lmProb = math.log(lm.probability(newTarget,history:_*))\n    val tmProb = math.log(tm.alpha((source(sourceIndex),newTarget))) + math.log(tm.beta((hyp.target .length, sourceIndex, targetLength, source.length)))\n    lmProb + tmProb\n  }\n  def append(hyp:Hypothesis) = { for (sourceIndex <- hyp.remaining; targetWord <- lm.vocab) yield \n    Hypothesis(targetWord :: hyp.target, \n               hyp.align + (sourceIndex -> hyp.target.length), \n               hyp.remaining - sourceIndex, \n               hyp.score + score(hyp,targetWord,sourceIndex), \n               Some(hyp))} \n  \n  var beam:Beam = List(Hypothesis(List(\"NULL\"), Map.empty, source.indices.toSet,  0.0))    \n  var history:List[Beam] = List(beam) \n  while (beam.head.remaining.nonEmpty) {\n    val withNewTarget = beam.flatMap(hyp => append(hyp)).distinct\n    beam = withNewTarget.sortBy(-_.score).take(beamSize).toList\n    history ::= beam\n  }  \n  history.map(_.map(h => h.copy(target = h.target.reverse))).reverse\n} ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \",\"val ibm2Alignments = ibm2Iterations.last._1\\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   \",\"def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2.beta(ti,si,5,4) \\nbarChart(distort(0), color = Some(\\\"#fc0\\\"))\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 43,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def toStringSeq(hyp:Hypothesis,source:IndexedSeq[String]) = \n  Seq(hyp.target.mkString(\" \"), source.indices.map(i => if (hyp.remaining(i)) \"_\" else source(i)).mkString(\" \"), hyp.remaining.size, hyp.score)    \n  \ndef renderBeam(beam:Beam, source:IndexedSeq[String]) = \n  table(beam.map(toStringSeq(_,source)))\n\ndef renderHistory(hist:List[Beam], source:IndexedSeq[String]) = \n  table(hist.flatMap(_.map(toStringSeq(_,source))))\n\n//renderBeam(List(Hypothesis(List(\"NULL\"), Map.empty, source.indices.toSet,  0.0)),source)    ",
      "extraFields" : {
        "hide" : "true",
        "hide_output" : "true",
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \",\"val ibm2Alignments = ibm2Iterations.last._1\\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   \",\"def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2.beta(ti,si,5,4) \\nbarChart(distort(0), color = Some(\\\"#fc0\\\"))\",\"case class Hypothesis(target:List[String], align:Map[Int,Int], remaining:Set[Int],\\n                      score:Double, parent:Option[Hypothesis]= None)\\n\\ntype Beam = List[Hypothesis]\\n \\ndef decodeModel2(tm:Model, lm:LanguageModel, \\n                 source:IndexedSeq[String], beamSize:Int) = {\\n  \\n  def score(hyp:Hypothesis,newTarget:String,sourceIndex:Int) = {\\n    val targetLength = source.length + 1\\n    val history = hyp.target.take(lm.order - 1).reverse\\n    val lmProb = math.log(lm.probability(newTarget,history:_*))\\n    println(\\\"History: \\\" + history)\\n    println(\\\"New Target: \\\" + newTarget)\\n    println(lmProb)\\n    val tmProb = math.log(tm.alpha((source(sourceIndex),newTarget))) + math.log(tm.beta((hyp.target .length, sourceIndex, targetLength, source.length)))\\n    lmProb + tmProb\\n  }\\n  def append(hyp:Hypothesis) = { for (sourceIndex <- hyp.remaining; targetWord <- lm.vocab) yield \\n    Hypothesis(targetWord :: hyp.target, \\n               hyp.align + (sourceIndex -> hyp.target.length), \\n               hyp.remaining - sourceIndex, \\n               hyp.score + score(hyp,targetWord,sourceIndex), \\n               Some(hyp))} \\n  \\n  var beam:Beam = List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0))    \\n  var history:List[Beam] = List(beam) \\n  while (beam.head.remaining.nonEmpty) {\\n    val withNewTarget = beam.flatMap(hyp => append(hyp)).distinct\\n    beam = withNewTarget.sortBy(-_.score).take(beamSize).toList\\n    history ::= beam\\n  }  \\n  history.map(_.map(h => h.copy(target = h.target.reverse))).reverse\\n} \"]"
      }
    }
  }, {
    "id" : 44,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nLet us test this decoder on a simple sentence, using a uniform language model.",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\ntrain.size\",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype Model = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):Model = {\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n}\\nval model = learn(train)\\nbarChart(model sortBy(_.tgt) map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\ndef decode(source:Sentence, model:Model, lm:LanguageModel) = {\\n  def score(param:Param) = param.prob * lm.probability(param.tgt)\\n  val source2targets = model groupBy (_.src) //create one sub-model per source word\\n  val source2Best = source2targets mapValues (m => (m maxBy score).tgt) //only keep best target\\n  val words = source.tokens map (s => source2Best(s.word))\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm)\\nrenderTokens(target) \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val train = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head)\\n\\nval targetVocab = train.flatMap(_._1.tokens.map(_.word)).toSet\\nval sourceVocab = train.flatMap(_._2.tokens.map(_.word)).toSet \",\"type Align = IndexedSeq[IndexedSeq[Double]]\\ncase class Model(\\n    alpha:Map[(String,String),Double],\\n    beta:Map[(Int,Int,Int,Int),Double]) \\n\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\\n\\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,train)\\nRenderer.renderWeightedAlignment(train.head._1, train.head._2, alignments.head)\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((a,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += a(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += a(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += a(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\nval theta1 = mStep(alignments,train)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"small\\\")) \",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1Iterations = emModel1(init, train, 10)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)\",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\"))\",\"Renderer.renderWeightedAlignment(train.head._1, train.head._2, ibm1Iterations.last._1.head)\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \\nval ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, train, 10)\\nval ibm2 = ibm2Iterations.last._2\\nibm2Iterations.last._3 \",\"def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2Iterations.last._2.beta(ti,si,5,4) \\nbarChart(distort(0), color = Some(\\\"#fc0\\\"))\",\"case class Hypothesis(target:List[String], align:Map[Int,Int], remaining:Set[Int],\\n                      score:Double, parent:Option[Hypothesis]= None)\\n\\ntype Beam = List[Hypothesis]\\n \\ndef decodeModel2(tm:Model, lm:LanguageModel, \\n                 source:IndexedSeq[String], beamSize:Int) = {\\n  \\n  def score(hyp:Hypothesis,newTarget:String,sourceIndex:Int) = {\\n    val targetLength = source.length + 1\\n    val history = hyp.target.take(lm.order - 1)\\n    val lmProb = math.log(lm.probability(newTarget,history:_*))\\n    val tmProb = math.log(tm.alpha((source(sourceIndex),newTarget))) + math.log(tm.beta((hyp.target .length, sourceIndex, targetLength, source.length)))\\n    lmProb + tmProb\\n  }\\n  def append(hyp:Hypothesis) = { for (sourceIndex <- hyp.remaining; targetWord <- lm.vocab) yield \\n    Hypothesis(targetWord :: hyp.target, \\n               hyp.align + (sourceIndex -> hyp.target.length), \\n               hyp.remaining - sourceIndex, \\n               hyp.score + score(hyp,targetWord,sourceIndex), \\n               Some(hyp))} \\n  \\n  var beam:Beam = List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0))    \\n  var history:List[Beam] = List(beam) \\n  while (beam.head.remaining.nonEmpty) {\\n    val withNewTarget = beam.flatMap(hyp => append(hyp)).distinct\\n    beam = withNewTarget.sortBy(-_.score).take(beamSize).toList\\n    history ::= beam\\n  }  \\n  history.map(_.map(h => h.copy(target = h.target.reverse))).reverse\\n}\",\"def toStringSeq(hyp:Hypothesis,source:IndexedSeq[String]) = \\n  Seq(hyp.target.mkString(\\\" \\\"), source.indices.map(i => if (hyp.remaining(i)) \\\"_\\\" else source(i)).mkString(\\\" \\\"), hyp.remaining.size, hyp.score)    \\n  \\ndef renderBeam(beam:Beam, source:IndexedSeq[String]) = \\n  table(beam.map(toStringSeq(_,source)))\\n\\ndef renderHistory(hist:List[Beam], source:IndexedSeq[String]) = \\n  table(hist.flatMap(_.map(toStringSeq(_,source))))\\n\\n//renderBeam(List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0)),source)    \"]"
      }
    }
  }, {
    "id" : 45,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val source = Vector(\"groß\", \"ist\", \"ein\", \"Mann\")\nval targetVocab = trainM2.flatMap(_._1.tokens.map(_.word)).toSet\nval lm = UniformLM(targetVocab - \"NULL\")\nval hist = decodeModel2(ibm2,lm, source, 2)\nrenderHistory(hist.take(5),source)  \n//ibm2.alpha.filter(_._1._1 == \"klein\")          ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \",\"val ibm2Alignments = ibm2Iterations.last._1\\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   \",\"def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2.beta(ti,si,5,4) \\nbarChart(distort(0), color = Some(\\\"#fc0\\\"))\",\"case class Hypothesis(target:List[String], align:Map[Int,Int], remaining:Set[Int],\\n                      score:Double, parent:Option[Hypothesis]= None)\\n\\ntype Beam = List[Hypothesis]\\n \\ndef decodeModel2(tm:Model, lm:LanguageModel, \\n                 source:IndexedSeq[String], beamSize:Int) = {\\n  \\n  def score(hyp:Hypothesis,newTarget:String,sourceIndex:Int) = {\\n    val targetLength = source.length + 1\\n    val history = hyp.target.take(lm.order - 1).reverse\\n    val lmProb = math.log(lm.probability(newTarget,history:_*))\\n    println(\\\"History: \\\" + history)\\n    println(\\\"New Target: \\\" + newTarget)\\n    println(lmProb)\\n    val tmProb = math.log(tm.alpha((source(sourceIndex),newTarget))) + math.log(tm.beta((hyp.target .length, sourceIndex, targetLength, source.length)))\\n    lmProb + tmProb\\n  }\\n  def append(hyp:Hypothesis) = { for (sourceIndex <- hyp.remaining; targetWord <- lm.vocab) yield \\n    Hypothesis(targetWord :: hyp.target, \\n               hyp.align + (sourceIndex -> hyp.target.length), \\n               hyp.remaining - sourceIndex, \\n               hyp.score + score(hyp,targetWord,sourceIndex), \\n               Some(hyp))} \\n  \\n  var beam:Beam = List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0))    \\n  var history:List[Beam] = List(beam) \\n  while (beam.head.remaining.nonEmpty) {\\n    val withNewTarget = beam.flatMap(hyp => append(hyp)).distinct\\n    beam = withNewTarget.sortBy(-_.score).take(beamSize).toList\\n    history ::= beam\\n  }  \\n  history.map(_.map(h => h.copy(target = h.target.reverse))).reverse\\n} \",\"def toStringSeq(hyp:Hypothesis,source:IndexedSeq[String]) = \\n  Seq(hyp.target.mkString(\\\" \\\"), source.indices.map(i => if (hyp.remaining(i)) \\\"_\\\" else source(i)).mkString(\\\" \\\"), hyp.remaining.size, hyp.score)    \\n  \\ndef renderBeam(beam:Beam, source:IndexedSeq[String]) = \\n  table(beam.map(toStringSeq(_,source)))\\n\\ndef renderHistory(hist:List[Beam], source:IndexedSeq[String]) = \\n  table(hist.flatMap(_.map(toStringSeq(_,source))))\\n\\n//renderBeam(List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0)),source)    \"]"
      }
    }
  }, {
    "id" : 46,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "<div class=\"newslide\"></div>\nThere are currently two contenders for the most likely translation. This is because the translation model is uncertain about the translation of \"groß\" which can be \"tall\" in the context of the height of humans, and \"big\" in most other settings. To avoid this uncertainty we can use a language model to capture the fact that \"man is big\" is a little less likely than \"man is tall\". <span class=\"summary\">We can reduce ambiguity by using a better *language model*</span>",
      "extraFields" : { }
    }
  }, {
    "id" : 47,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val lmTrainSet = trainM2.flatMap(_._1.tokens.map(_.word))\nval lm2 = LaplaceLM(NGramLM(lmTrainSet.toIndexedSeq, 3),0.1)\nval hist2 = decodeModel2(ibm2,lm2, source, 2)\nrenderHistory(hist2.take(5),source) \n//lm2.probability(\"tall\", \"man\", \"is\") ",
      "extraFields" : {
        "aggregatedCells" : "[\"val train = Seq(\\n  \\\"the house is small\\\" -> \\\"das Haus ist klein\\\",\\n  \\\"the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"a man is tall\\\" -> \\\"ein Mann ist groß\\\",\\n  \\\"my house is small\\\" -> \\\"klein ist mein Haus\\\"\\n) map (p => segment(p._1).sentences.head -> segment(p._2).sentences.head)\\ntrain.size \",\"import scala.collection.mutable.HashMap\\ncase class Param(src:String, tgt: String, prob: Double)\\ntype NaiveModel = Seq[Param]\\ndef learn(data:Seq[(Sentence,Sentence)]):NaiveModel = {\\n  //data structures to store counts in nominator and denominator\\n  val norm = new HashMap[String,Double] withDefaultValue 0.0\\n  val counts = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  //do the counting\\n  for ((target,source) <- data) {\\n    for (i <- 0 until target.tokens.length) {\\n      norm(target.tokens(i).word) += 1.0\\n      counts(source.tokens(i).word -> target.tokens(i).word) += 1.0\\n    }    \\n  }\\n  //convert the map to a sequence of parameters\\n  counts.toSeq map {case ((source,target),count) => Param(source,target,count/norm(target)) }\\n} \",\"val model = learn(train)\\nbarChart(model map (p => (p.src,p.tgt) -> p.prob) filter (_._1._2 == \\\"is\\\")) \",\"import uk.ac.ucl.cs.mr.statnlpbook.chapter.languagemodels._\\n\\ndef decode(source:Sentence, model:NaiveModel, lm:LanguageModel) = {\\n  //create a mapping from source words to relevant parameters\\n  val source2targets = model groupBy (_.src) \\n  def translate(token:Token) = {\\n    val candidates:Seq[Param] = source2targets(token.word)\\n    val scoredByLM = candidates map (p => p.copy(prob = p.prob * lm.probability(p.tgt)))\\n    scoredByLM.maxBy(_.prob).tgt\\n  }\\n  //translate word-by-word (for which LM does this work?)\\n  val words = source.tokens map translate\\n  Document(IndexedSeq(words))\\n}\\nval source = train(1)._2 //\\\"klein ist das Haus\\\"\\nval lm = UniformLM(model.map(_.tgt).toSet)\\nval target = decode(source, model, lm) \\ntarget.toText \",\"val target = segment(\\\"NULL the house is small\\\").sentences(0)\\nval source = segment(\\\"klein ist das Haus\\\").sentences(0)\\nRenderer.renderAlignment(target,source,Seq(1->2,2->3,3->1,4->0))\",\"val target2 = segment(\\\"NULL 家 わ 小さいい です\\\").sentences(0)\\nval source2 = segment(\\\"The house is small\\\").sentences(0)\\nRenderer.renderAlignment(target2,source2,Seq(0 -> 0, 1 -> 1, 3 -> 3, 4 -> 2))\",\"val trainM2 = Seq(\\n  \\\"NULL the house is small\\\" -> \\\"klein ist das Haus\\\",\\n  \\\"NULL a man is tall\\\" -> \\\"groß ist ein Mann\\\",\\n  \\\"NULL my house is small\\\" -> \\\"klein ist mein Haus\\\",\\n  \\\"NULL the building is big\\\" -> \\\"groß ist das Gebäude\\\",\\n  \\\"NULL the building is long\\\" -> \\\"lang ist das Gebäude\\\"\\n) map (p => segment(p _1).sentences.head -> segment(p _2).sentences.head) \",\"//Each element in the sequence corresponds to one source token,\\n//and each element in the nested sequence to a target token.\\ntype Align = IndexedSeq[IndexedSeq[Double]]\\n\\n//Model parameters\\ncase class Model(\\n  alpha:Map[(String,String),Double],\\n  beta:Map[(Int,Int,Int,Int),Double]) \\n\\n//normalizing a distribution\\ndef norm(v:IndexedSeq[Double]) = { val s = v.sum; v map (_ / s) } \\n\\n//estimate new soft alignments\\ndef eStep(model:Model, data:Seq[(Sentence,Sentence)]):Seq[Align] = {\\n  for ((target,source) <- data) yield {\\n    def score(si:Int, ti:Int) = \\n      model.alpha(source.tokens(si).word,target.tokens(ti).word) * \\n      model.beta(ti,si,target.tokens.length, source.tokens.length)\\n    for (si <- source.tokens.indices) yield \\n      norm(for (ti <- target.tokens.indices) yield score(si,ti))\\n  }\\n}\",\"val sourceVocab = trainM2.flatMap(_._2.tokens.map(_.word)).toSet \\nval init = Model(\\n    Map.empty withDefaultValue (1.0 / sourceVocab.size),\\n    Map.empty withDefault {case (ti,si,lt,ls) => 1.0 / lt })\\n\\nval alignments = eStep(init,trainM2) \\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, alignments(0))\",\"def mStep(aligns:Seq[Align],data:Seq[(Sentence,Sentence)]) = {\\n  val alpha = new HashMap[(String,String),Double] withDefaultValue 0.0\\n  val alphaNorm = new HashMap[String,Double] withDefaultValue 0.0\\n  val beta = new HashMap[(Int,Int,Int,Int),Double] withDefaultValue 0.0\\n  val betaNorm = new HashMap[(Int,Int,Int),Double] withDefaultValue 0.0\\n  for ((pi,(t,s)) <- aligns zip data) {\\n    for (ti <- t.tokens.indices; si <- s.tokens.indices) {\\n      alpha(s.tokens(si).word,t.tokens(ti).word) += pi(si)(ti) \\n      alphaNorm(t.tokens(ti).word) += pi(si)(ti) \\n      beta(ti,si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n      betaNorm(si,t.tokens.length,s.tokens.length) += pi(si)(ti)\\n    }  \\n  }                 \\n  Model(\\n    alpha.toMap map {case ((s,t),p) => ((s,t), p / alphaNorm(t))} withDefaultValue 0.0,\\n    beta.toMap map {case ((ti,si,ls,lt),p) => ((ti,si,ls,lt),p / betaNorm(si,ls,lt))} withDefaultValue 0.0)    \\n}\\n\",\"val theta1 = mStep(alignments,trainM2)\\nbarChart(theta1.alpha.filter(_._1._2 == \\\"is\\\"))\",\"def change(a1:Seq[Align],a2:Seq[Align]) = {\\n  val flatA1 = a1.flatMap(_.flatten) \\n  val flatA2 = a2.flatMap(_.flatten)\\n  val diffs = (flatA1 zip flatA2) map {case(p1,p2) => math.abs(p1 - p2)}\\n  diffs.sum / flatA1.length\\n}\\ndef emModel1(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data).copy(beta = init.beta) \\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1Iterations = emModel1(init, trainM2, 100)\\nval xyData = ibm1Iterations.indices.map(_.toDouble) -> ibm1Iterations.map(_._3) \\nlineplot(xyData)  \",\"barChart(ibm1Iterations.last._2.alpha.filter(_._1._2 == \\\"small\\\")) \",\"Renderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm1Iterations.last._1(0))\",\"def emModel2(init:Model, data:Seq[(Sentence,Sentence)], iterations:Int) = {\\n  var model = init\\n  var alignments = data.map(d => d._2.tokens.map(t => d._1.tokens.map(_ => 0.0)))\\n  for (_ <- 0 until iterations) yield {\\n    val old = alignments\\n    alignments = eStep(model, data)\\n    model = mStep(alignments, data)\\n    (alignments,model,change(old,alignments))\\n  }                   \\n} \",\"val ibm1 = ibm1Iterations.last._2\\nval ibm2Iterations = emModel2(ibm1, trainM2, 100)\\nval ibm2 = ibm2Iterations.last._2\\nbarChart(ibm2.alpha.filter(_._1._2 == \\\"house\\\")) \",\"val ibm2Alignments = ibm2Iterations.last._1\\nRenderer.renderWeightedAlignment(trainM2(0)._1, trainM2(0)._2, ibm2Alignments(0))   \",\"def distort(si:Int) = for (ti <- 0 until 5) yield ti -> ibm2.beta(ti,si,5,4) \\nbarChart(distort(0), color = Some(\\\"#fc0\\\"))\",\"case class Hypothesis(target:List[String], align:Map[Int,Int], remaining:Set[Int],\\n                      score:Double, parent:Option[Hypothesis]= None)\\n\\ntype Beam = List[Hypothesis]\\n \\ndef decodeModel2(tm:Model, lm:LanguageModel, \\n                 source:IndexedSeq[String], beamSize:Int) = {\\n  \\n  def score(hyp:Hypothesis,newTarget:String,sourceIndex:Int) = {\\n    val targetLength = source.length + 1\\n    val history = hyp.target.take(lm.order - 1).reverse\\n    val lmProb = math.log(lm.probability(newTarget,history:_*))\\n    val tmProb = math.log(tm.alpha((source(sourceIndex),newTarget))) + math.log(tm.beta((hyp.target .length, sourceIndex, targetLength, source.length)))\\n    lmProb + tmProb\\n  }\\n  def append(hyp:Hypothesis) = { for (sourceIndex <- hyp.remaining; targetWord <- lm.vocab) yield \\n    Hypothesis(targetWord :: hyp.target, \\n               hyp.align + (sourceIndex -> hyp.target.length), \\n               hyp.remaining - sourceIndex, \\n               hyp.score + score(hyp,targetWord,sourceIndex), \\n               Some(hyp))} \\n  \\n  var beam:Beam = List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0))    \\n  var history:List[Beam] = List(beam) \\n  while (beam.head.remaining.nonEmpty) {\\n    val withNewTarget = beam.flatMap(hyp => append(hyp)).distinct\\n    beam = withNewTarget.sortBy(-_.score).take(beamSize).toList\\n    history ::= beam\\n  }  \\n  history.map(_.map(h => h.copy(target = h.target.reverse))).reverse\\n} \",\"def toStringSeq(hyp:Hypothesis,source:IndexedSeq[String]) = \\n  Seq(hyp.target.mkString(\\\" \\\"), source.indices.map(i => if (hyp.remaining(i)) \\\"_\\\" else source(i)).mkString(\\\" \\\"), hyp.remaining.size, hyp.score)    \\n  \\ndef renderBeam(beam:Beam, source:IndexedSeq[String]) = \\n  table(beam.map(toStringSeq(_,source)))\\n\\ndef renderHistory(hist:List[Beam], source:IndexedSeq[String]) = \\n  table(hist.flatMap(_.map(toStringSeq(_,source))))\\n\\n//renderBeam(List(Hypothesis(List(\\\"NULL\\\"), Map.empty, source.indices.toSet,  0.0)),source)    \",\"val source = Vector(\\\"groß\\\", \\\"ist\\\", \\\"ein\\\", \\\"Mann\\\")\\nval targetVocab = trainM2.flatMap(_._1.tokens.map(_.word)).toSet\\nval lm = UniformLM(targetVocab - \\\"NULL\\\")\\nval hist = decodeModel2(ibm2,lm, source, 2)\\nrenderHistory(hist.take(5),source)  \\n//ibm2.alpha.filter(_._1._1 == \\\"klein\\\")          \"]"
      }
    }
  }, {
    "id" : 48,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Note that \"a man is tall\" is also more likely in the [Google N-grams corpus](https://books.google.com/ngrams/graph?content=a+man+is+tall%2C+a+man+is+big&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Ca%20man%20is%20tall%3B%2Cc0%3B.t1%3B%2Ca%20man%20is%20big%3B%2Cc0).\n\n<div class=\"newslide\"></div>\n### Summary\nThere are a few high level messages to take away from this chapter.\n\n* MT is an instance structured prediction recipe\n* The noisy channel is one modelling framework\n* word-based MT is foundation and blue print for more complex models\n* Training with EM\n* NLP Tricks: \n    * introducing latent alignment variables to simplify problem\n    * decoding with Beams\n\n### Background Material\n* [Lecture notes on IBM Model 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf) of Mike Collins.  \n* Jurafsky & Martin, Speech and Language Processing: \n    * Chapter 26, Machine Translation.\n    * Chapter 6, EM Algorithm\n\n\n\n",
      "extraFields" : { }
    }
  } ],
  "config" : {
    "autosave" : "false"
  }
}
